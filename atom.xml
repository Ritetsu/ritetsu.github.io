<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://li-zhe.com/"/>
  <updated>2020-01-03T06:36:07.219Z</updated>
  <id>http://li-zhe.com/</id>
  
  <author>
    <name>LiZhe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2019年的总结&amp;2020年的FLAG</title>
    <link href="http://li-zhe.com/posts/3590534990/"/>
    <id>http://li-zhe.com/posts/3590534990/</id>
    <published>2019-12-31T15:59:59.000Z</published>
    <updated>2020-01-03T06:36:07.219Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>入夜莫名焦虑开始怀(自)旧(黑)。进入大学以来我好像一直在忙各种各样的事情，习惯了独来独往之后才发现更喜欢那种身边都是熟悉的人的感觉。有时候真想回到初三把所有人的同学录都好好地再写一遍，而不是草草地“少年珍重！”四个大字结束，那真的是我最不走心，也最开心的三年了。高中的日子快得来不及回想，但唯独那些人的笑容我竟也还记得清楚。<br>2018年，我一定要过得更深刻一些。</p></blockquote><p>以上是2017年1月8日我写下的总结，回首望去，原来3年前我就意识到了自己不是真的喜欢孤独以及常伴吾身的焦虑，但却直到现在才开始正视它们。</p><p>太惭愧了。</p><h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><h2 id="1-N2测试：有惊无险；"><a href="#1-N2测试：有惊无险；" class="headerlink" title="1.N2测试：有惊无险；"></a>1.N2测试：有惊无险；</h2><p>尽管听听力时满头问号的感觉记忆犹新，但现在我有底气对每一个想考N2的人轻描淡写地说“N2不难”了；也可以（光明正大地）对知乎上那些回答“如何x天内零基础考过N2”然后疯狂卖资料或者给公众号/app打广告的答主们嗤之以鼻了。</p><p>（当然不管我考没考过，我都对这种假装好意分享实则浪费小白时间的人感到不屑）</p><p>（对于那种从来不主动学日语，但是到处搜集“资料”“秘籍”并自认行家的蠢人我也感到遗憾）</p><p>写到这不禁想起之前在B站看到的“总有人看了几集动漫就以为自己会日语了”的视频，我他娘的还真有这种朋友，成天なにそっかなるほど挂在嘴边，真跟他说起日语来又只能装死。不过毕竟是兄弟，也就图一乐，要是不熟的人操着塑料日语来跟我装，我是<del>（直接突突了）</del>根本不会理睬的。</p><p>可惜，N2考前的那段时光恐怕就是我迄今的日语水平巅峰了，毕竟双学位结课后基本上没什么机会<del>（炫耀）</del>用到日语。</p><p>赶紧把去日本旅游提上日程，争取好好锻炼一下口语。</p><h2 id="2-日语双学位：后知后觉；"><a href="#2-日语双学位：后知后觉；" class="headerlink" title="2. 日语双学位：后知后觉；"></a>2. 日语双学位：后知后觉；</h2><p>双学位的课程顺利结束了，而我一如既往地在彻底结束后一段时间才真切地意识到这件事情。</p><p>当周六早上顺着生物钟醒来却发现没有课可以上，当在食堂里和很久不见的只有在双学位课上才有机会多看几眼的同学相遇，当瞥见书桌上的日语教材和笔记却突然回想不起这是哪一门课上留下的记录时，我才突然反应过来，和一群在某些亚文化圈有高度重合的兴趣的人们就此别过了。</p><p>出于胆怯<del>（本来想写礼貌，考虑到在批判自己，还是真切一点好）</del>，我没能和其中感兴趣的几个人建立更深的联系，以后只能做朋友圈的点赞之交，属实是今年的一大遗憾。</p><h2 id="3-自学生信：重大抉择；"><a href="#3-自学生信：重大抉择；" class="headerlink" title="3. 自学生信：重大抉择；"></a>3. 自学生信：重大抉择；</h2><p>不得不说学院把《生物信息学》排在大三下学期是真的坑。</p><p>为了不在夏令营时只能和老师谈情怀和说“我不会但我可以学”这种p话，我在寒假回家后紧急开始了自学。毫无生信基础的我一开始学得一头雾水，一个隐马尔可夫模型看了几天都看不明白，导致寒假一度在家学到自闭。</p><p>开学后试图去北大做科研训练，但仅凭一个寒假的入门性学习我还是没能跟上师兄师姐们嘴里的深度学习和卷积神经网络。</p><p>受到打击的我在最后一个学期抽出时间学了很多东西，包括生信/R/Python/机器学习甚至C语言。下定决心花时间去学算是我做出的重大抉择了，当然世界线的变动可能在几个月之前的某堂课上，那又是另一个故事了。</p><h2 id="4-夏令营：一言难蔽；"><a href="#4-夏令营：一言难蔽；" class="headerlink" title="4. 夏令营：一言难蔽；"></a>4. 夏令营：一言难蔽；</h2><p>我为什么放弃出国留学，为什么没去TOP2，又为什么在高校和中科院之间选择了后者，以及各种对三观造成冲击的遭遇。</p><p>这其中的每一件事细说起来都能写出几千字的长文，考虑到最终的结果还没有完全确定，这一段就先作罢。</p><h2 id="5-人际交往：一塌糊涂；"><a href="#5-人际交往：一塌糊涂；" class="headerlink" title="5. 人际交往：一塌糊涂；"></a>5. 人际交往：一塌糊涂；</h2><p>我没有毁掉任何已有的人际关系。我相信我和朋友们的友谊依旧坚如磐石。</p><p>一塌糊涂的是我在这方面几乎没有成长，没有主动地去结识感兴趣的人。换而言之，我在不那么熟悉的人面前还是放不开，感觉像是一种极度扭曲的“偶像光环”。当然我不是偶像，也深知没有人会特别关注到我。但我心里总觉得与其留下不好的奇怪的印象，倒不如不去过多地展现自己。这种想法在形式上的体现是日常生活中的独行者，朋友圈里不留痕迹的过客；造成的结果是主动斩断了自己为数不多的社交。</p><p>当我数次想出去吃东西或者去哪玩玩逛逛，却只能想出几个屈指可数的人选，而他们偏偏还不想去时，我觉得2020年我需要在这方面做出一些改变。</p><h2 id="6-异性关系：注孤生；"><a href="#6-异性关系：注孤生；" class="headerlink" title="6. 异性关系：注孤生；"></a>6. 异性关系：注孤生；</h2><p>真的，我兴许就是初中遇见太多惯着我的可爱的人，高中又幸运地结识并发展了初恋，耗尽了异性缘；大学三年愣是没有什么故事可说。虽是被人表白过一次，但是回想起事情发生的突然和人设的巨大反差，我总怀疑这件事并没有发生过。</p><p>一边有所期待有所企图，一边却又不愿意将就、不敢<del>（不愿意）</del>“屈尊”主动，说是注孤生应该没有毛病。</p><h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><p>喜闻乐见的立FLAG环节。</p><h3 id="去日本，全程只允许自己说日语（感觉大概率因为攒不住💴而失败）；"><a href="#去日本，全程只允许自己说日语（感觉大概率因为攒不住💴而失败）；" class="headerlink" title="去日本，全程只允许自己说日语（感觉大概率因为攒不住💴而失败）；"></a>去日本，全程只允许自己说日语<del>（感觉大概率因为攒不住💴而失败）</del>；</h3><h3 id="结识1个异性朋友，能一起为了看电影而去看电影的那种；"><a href="#结识1个异性朋友，能一起为了看电影而去看电影的那种；" class="headerlink" title="结识1个异性朋友，能一起为了看电影而去看电影的那种；"></a>结识1个异性朋友，能一起为了看电影而去看电影的那种；</h3><h3 id="做事情“莽”一些，遵从内心最初的想法而不是思来想去得一个“看上去最优”的解（简而言之，让谦让和谦虚少一点，该秀的时候要秀一下的呀）；"><a href="#做事情“莽”一些，遵从内心最初的想法而不是思来想去得一个“看上去最优”的解（简而言之，让谦让和谦虚少一点，该秀的时候要秀一下的呀）；" class="headerlink" title="做事情“莽”一些，遵从内心最初的想法而不是思来想去得一个“看上去最优”的解（简而言之，让谦让和谦虚少一点，该秀的时候要秀一下的呀）；"></a>做事情“莽”一些，遵从内心最初的想法而不是思来想去得一个“看上去最优”的解（简而言之，让谦让和谦虚少一点，该秀的时候要秀一下的呀）；</h3><p>至于几点前睡觉，睡前不玩手机，xx考多少分之类的就不立了，感觉没什么意思；</p><p>后续想到新的FLAG再补充，就先这样吧~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;入夜莫名焦虑开始怀(自)旧(黑)。进入大学以来我好像一直在忙各种各样的事情，习惯了独来独往之后才发现更喜欢那种身边都是熟悉的人的感觉。有时候真想回到初三把所有人的同学录都好好地再写一遍，而不是草草地“少年珍重！”四个大字结束，那真的是我最不走心，也
      
    
    </summary>
    
    
      <category term="随笔" scheme="http://li-zhe.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="随笔" scheme="http://li-zhe.com/tags/%E9%9A%8F%E7%AC%94/"/>
    
      <category term="日常" scheme="http://li-zhe.com/tags/%E6%97%A5%E5%B8%B8/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(6)--推荐系统(Recommender Systems)及大规模机器学习</title>
    <link href="http://li-zhe.com/posts/1648416303/"/>
    <id>http://li-zhe.com/posts/1648416303/</id>
    <published>2019-12-21T04:17:34.000Z</published>
    <updated>2020-01-02T01:14:07.377Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第6篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><p>想想网易云音乐是如何进行每日推荐？这就是推荐系统。</p><h3 id="基于内容的推荐系统"><a href="#基于内容的推荐系统" class="headerlink" title="基于内容的推荐系统"></a>基于内容的推荐系统</h3><p>假设有不同部电影，有不同的人给他们打分，每部电影有不同的特征（如2个）【向量形式】；如何基于这些信息来构建一个推荐系统算法呢？——假设用线性回归模型：</p><p>$\theta^{(j)}$是用户j的参数向量（打分情况）</p><p>$x^{(i)}$是电影i的特征向量（浪漫程度、动作程度等）</p><p>对于用户j和电影i，我们预测评分为：</p><script type="math/tex; mode=display">(\theta^{(i)})^Tx^{(i)}</script><p>之后参照线性回归和梯度下降求解。</p><h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><p>如果我们没有用户的参数（评分），也没有电影的特征（浪漫程度、动作程度），可以通过协同过滤算法同时学习两者：</p><p>优化目标（代价函数）更改为同时针对x和$\theta$进行，具体步骤如下：</p><p>1.初始的x和$\theta$为一些随机的小值；</p><p>2.使用梯度下降算法最小化代价函数；</p><p>3.训练完算法后，预测用户j给电影i的评分。</p><h3 id="向量化：低秩矩阵分解（？）"><a href="#向量化：低秩矩阵分解（？）" class="headerlink" title="向量化：低秩矩阵分解（？）"></a>向量化：低秩矩阵分解（？）</h3><h3 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h3><p>当某用户没有任何基础参数时，怎么办呢？→对所有数据进行均值归一化处理（减去均值），利用处理后的新矩阵来训练算法，用新训练出的算法预测评分后，将平均值加回去！即预测：</p><script type="math/tex; mode=display">(\theta^{(i)})^T+\mu_i</script><h2 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h2><p>当有大量数据（如1000w条）作为训练集时，我们如何应对？</p><p>可以作出交叉验证集的代价函数和训练集代价函数随着训练样本数增加的变化曲线，或许只需要1000个训练集就能取得很好的结果。</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>将代价函数定义为一个单一训练实例的代价：</p><script type="math/tex; mode=display">cost(\theta,(x^{(i)},y^{(i)})=\cfrac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2</script><p><strong>随机</strong>的意思即，对训练集随机“洗牌”，然后：</p><script type="math/tex; mode=display">Repeat(usually 1-10){   for i=1:m{       \theta:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}     (for j=0:n)   }}</script><p>其好处在于每一次计算后都更新$\theta$，而不需将大量训练集求和。缺点是，并不是每一步都朝正确的方向走去，甚至有可能无法走到最小值的那一点，而是在最小值点附近徘徊。</p><h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>介于批量梯度下降和随机下降算法之间，每计算常数b次训练实例，便更新一次参数$\theta$，一般令b=2-100，从而可以用项量化的方式来循环b个训练实例。</p><h3 id="随机梯度下降收敛"><a href="#随机梯度下降收敛" class="headerlink" title="随机梯度下降收敛"></a>随机梯度下降收敛</h3><p>在随机梯度下降中，每一次更新$\theta$之前都计算一次代价，然后每x次迭代后，求出这x次对训练实例计算代价的平均值，然后绘制这些平均值与x迭代的次数之间的函数图表。</p><p>1.函数图像颠簸不平，但<strong>不明显减少</strong>：考虑增大$\alpha$</p><p>2.函数图像<strong>不断上升</strong>：考虑减小$\alpha$，如令学习率随迭代次数的增加而减小，比如：</p><script type="math/tex; mode=display">\alpha=\cfrac{const1}{iterationNumber+const2}</script><h3 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h3><p>有连续的数据流，而非静态的数据集。</p><p>因而也是对一个单一实例进行训练，而非训练集！且一旦一个数据学习完成了，可以丢弃之。</p><h3 id="映射化简和数据并行"><a href="#映射化简和数据并行" class="headerlink" title="映射化简和数据并行"></a>映射化简和数据并行</h3><p>把数据集分配给多台计算机，每个计算机求一个子集，再将结果汇总。（或者利用多核CPU的多个核心进行并行运算）<br>假设400个训练实例被分配到4台计算机进行处理，最后</p><script type="math/tex; mode=display">\theta_j=\theta_j-\alpha\cfrac{1}{400}(temp^{(1)})(temp^{(2)})(temp^{(3)})(temp^{(4)})</script><h2 id="应用实例：图片文字识别"><a href="#应用实例：图片文字识别" class="headerlink" title="应用实例：图片文字识别"></a>应用实例：图片文字识别</h2><p>从图片中识取文字并辨认出来：</p><p>1.文字侦测——将图片上的文字与其他环境对象分离</p><p>2.字符切分——将文字分割成一个个单一的字符</p><p>3.字符分类——确定每一个字符是什么</p><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>按比例框定窗口，按比例放大缩小扫描整张图片。交给模型进行评判。完成识别后，还可以将识别区域进行扩展。</p><h3 id="获得大量数据和人工数据"><a href="#获得大量数据和人工数据" class="headerlink" title="获得大量数据和人工数据"></a>获得大量数据和人工数据</h3><p>人工数据——如识别文字，可以用字体库+背景自己创造数据</p><p>已有数据修改——如文字图片的扭曲、旋转和模糊处理</p><p>手动标记或众包</p><h3 id="上限分析"><a href="#上限分析" class="headerlink" title="上限分析"></a>上限分析</h3><p>在改进算法的过程中，假设某一个步骤（人工地使之）完全正确，对比不人工处理情况下的预测准确率，对不同的步骤都进行处理，选择改进空间最大的着力改进。</p><p>—</p><p>以上，机器学习的笔记暂时告一段落。这些都是暑假后半段自学的内容，再输出一遍同时自己也复习一下。如果有补充可能会直接文内更新/在新文章里加对应tag。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第6篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="AndrewNg《MachineLearning》笔记" scheme="http://li-zhe.com/categories/AndrewNg%E3%80%8AMachineLearning%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://li-zhe.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="大规模机器学习" scheme="http://li-zhe.com/tags/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(5)--无监督学习(Unsupervised Learning)及异常检测(Outlier Detection)</title>
    <link href="http://li-zhe.com/posts/1635273383/"/>
    <id>http://li-zhe.com/posts/1635273383/</id>
    <published>2019-12-20T02:17:34.000Z</published>
    <updated>2020-01-02T01:14:42.482Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第5篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><h3 id="K-means聚类"><a href="#K-means聚类" class="headerlink" title="K-means聚类"></a>K-means聚类</h3><p>首先选择K个随机的点，为<strong>聚类中心</strong>，然后对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点归为一类。再计算每个类的平均值，把该类的中心点移动到平均值的位置，重复对数据点进行聚类。——直到中心点不再变化。</p><h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>随机选择K&lt;m（训练集实例数量）个训练实例，然后令K个聚类中心分别与这K个训练实例相等。</p><p>为了解决局部最小值问题，通常进行多次K-means运算，每一次都重新进行随机初始化，最后再比较结果。</p><h4 id="选择聚类数（肘部法则）"><a href="#选择聚类数（肘部法则）" class="headerlink" title="选择聚类数（肘部法则）"></a>选择聚类数（肘部法则）</h4><p>更改K的值，直到代价函数下降较慢。（类似PCA碎石图）</p><h3 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h3><p>从二维降到一维 → 如将二元坐标点投影到一条线段上。</p><p>从三维到二维 → 如将三元坐标点投影到一个二维平面上。</p><p>数据可视化？ → 新产生特征的意义需要自己去发现。</p><h4 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h4><p>第一步，均值归一化，计算出所有特征的均值，然后令$x_j=x_j-\mu_j$，如果特征在不同数量级上，还需要除以标准差$\sigma^2$。</p><p>第二步，计算协方差矩阵：</p><script type="math/tex; mode=display">\Sigma=\cfrac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T</script><p>第三步，计算协方差矩阵的特征向量：</p><script type="math/tex; mode=display">[U,S,V] = svd(\Sigma)</script><p>U即是一个具有与数据之间最小投射误差的方向向量构成的矩阵，若想将数据从n维降到k维，只需从U中选取前k个向量，得到一个新的n×k的矩阵用$U_reduce$表示，然后计算得新的特征向量：</p><script type="math/tex; mode=display">z^{(i)}=U_{reduce}^T*x^{(i)}</script><p>x是n×1维的，所以结果为k×1维。</p><p>而S是一个n×n的矩阵，只有对角线上有值，可以用来计算平均均方误差与训练集方差的比例（小于1%为例）：</p><script type="math/tex; mode=display">\cfrac{\sum_{i=1}^{k}S_{ii}}{\sum_{i=1}^{n}S_{ii}}>=0.99</script><p>压缩数据后，近似地获得原有特征：</p><script type="math/tex; mode=display">x^{(i)}_{approx}=U_{reduce}^T*z^{(i)}</script><h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p>给定数据集，已知数据集正常，那么如何判断一个新的数据是不是异常值，以及这个新数据不属于这一组数据的几率如何？</p><h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h3><p>高斯分布即正态分布，符合之的变量$x~N(\mu,\sigma^2)$,其概率密度函数为：</p><script type="math/tex; mode=display">p(x,\mu,\sigma^2)=\cfrac{1}{\sqrt{2\pi}\sigma}exp{-\cfrac{(x-\mu)^2}{2\sigma^2}}</script><p>其中的$\mu$和$\sigma^2$分别为均值和方差，$\mu$影响中点位置，$\sigma^2$影响峰的宽度高度。</p><h3 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h3><p>给定数据集$x^{(1)}-x^{(m)}-$，则针对每一个特征计算mu和sigma的估计值：</p><script type="math/tex; mode=display">\mu_j=\cfrac{1}{m}\sum_{i=1}^{m}x_j^{(i)}</script><script type="math/tex; mode=display">\sigma_j^2=\cfrac{1}{m}\sum_{i=1}^{m}(x_j^(i)-\mu_j)^2</script><p>获得两个估计值后，和给定的新训练实例$x_j$一起代入高斯分布计算p(x)：</p><script type="math/tex; mode=display">p(x)=\prod^{n}_{j=1}p(x_j;\mu_j;\sigma_j^2)</script><p>当$p(x)&lt;\epsilon$时即为<strong>异常</strong>。</p><p>注意：异常检测与监督学习有些相似，但最大的不同在于异常检测的正向类（异常数据y=1）非常少，负向类非常多（正常数据y=0）！而监督学习同时拥有大量的正和负类。</p><h3 id="开发异常检测系统"><a href="#开发异常检测系统" class="headerlink" title="开发异常检测系统"></a>开发异常检测系统</h3><p>作为一个非监督学习算法，在用其开发一个异常检测系统时，需要注意数据的选取：</p><p>如有10000个正常数据，其中有20个异常，那么建议如下<strong>分配</strong>：</p><p>6000个正常数据作为训练集；</p><p>2000个正常数据和10个异常数据作为交叉检验集；</p><p>2000个正常数据和10个异常数据作为测试集。</p><p>而<strong>评价方法</strong>如下：</p><p>1.根据训练集数据，估计特征的均值和方差，并构建p(x)；</p><p>2.对交叉检验集，用不同的$\epsilon$值作阈值，预测数据是否异常；并根据F1选择$\epsilon$；</p><p>3.选出$\epsilon$后，针对测试集进行预测，计算F1值。</p><h3 id="选择特征"><a href="#选择特征" class="headerlink" title="选择特征"></a>选择特征</h3><p>如果数据分布不是高斯分布，可以通过对数变换或者指数变换使之成为高斯分布。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第5篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="AndrewNg《MachineLearning》笔记" scheme="http://li-zhe.com/categories/AndrewNg%E3%80%8AMachineLearning%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://li-zhe.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="异常检测" scheme="http://li-zhe.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(4)--支持向量机(Support Vector Machine)</title>
    <link href="http://li-zhe.com/posts/4293844555/"/>
    <id>http://li-zhe.com/posts/4293844555/</id>
    <published>2019-12-19T02:44:34.000Z</published>
    <updated>2020-01-02T01:13:59.042Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第4篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h2><h3 id="从逻辑回归开始"><a href="#从逻辑回归开始" class="headerlink" title="从逻辑回归开始"></a>从逻辑回归开始</h3><p>逻辑回归的假设：</p><script type="math/tex; mode=display">h_\theta(x)=\cfrac{1}{1+e^{-\theta^Tx}}</script><p>若y=1，我们希望$h_\theta(x)\approx1$，$\theta^Tx\gg0$</p><p>若y=0，则$h_\theta(x)\approx0$，$\theta^Tx\ll0$（注意考虑S型的函数）</p><p>考虑到代价函数：</p><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><p>y=0或y=1时代价函数会消掉一部分，导致$\theta^Tx$极大或极小能确保代价函数趋于0。</p><p>据此构建支持向量机：</p><script type="math/tex; mode=display">\underbrace{min}_{\theta}C\sum^{m}_{i=1}[y^{(i)}cost_1(\theta^Tx^{(i)}+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\cfrac{1}{2}\sum_{j=1}^{n}\theta_j^2</script><p>此处的C，相当于之前的$\cfrac{1}{\lambda}$，因此：</p><p>C较大时，相当于$\lambda$较小，可能导致过拟合，高方差；</p><p>C较小时，相当于$\lambda$较大，可能导致低拟合，高偏差。</p><p>向量u的范数=$\sqrt{u_1^2+u_2^2}$，代表u的欧几里得长度。</p><p>通过向量内积：</p><script type="math/tex; mode=display">\vec u^T\vec v=\vec v^T\vec u=||u||p</script><p>注意p和$||u||$都是实数，p作为投影是有方向的（正负）。</p><p>从而实现：</p><script type="math/tex; mode=display">\theta^Tx^{(i)}=p^{(i)}·||\theta||</script><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>如果说模型是：</p><script type="math/tex; mode=display">\theta_0+\theta_1x_1+\theta_2x_2+...\theta_4x1^2+...</script><p>那么我们可以用一系列新的特征f来替换模型中的每一项，如：</p><script type="math/tex; mode=display">f_1=x_1,f_2=x_2,f_3=x_1^2...</script><p>然而除了对原有特征进行组合以外，我们还可以用<strong>核函数</strong>来构建新的特征。</p><p>给定一个训练实例x，我们可以利用x的各个特征与我们预先选定的landmarks，$l^{1},l^{(2)},l^{(3)}$的近似程度来选新的特征$f_1,f_2,f_3$，例如：</p><script type="math/tex; mode=display">f_1=similarity(x,l^{(1)})=e(-\cfrac{||x-l^{(1)}||^2}{2\sigma^2})</script><p>其中：</p><script type="math/tex; mode=display">||x-l^{(1)}||^2=\sum_{j=1}^{n}(x_j-l_j^{(1)})^2</script><p>即x中的所有特征与$l^{(1)}$的距离之和，上面的similarity就是一个高斯核函数。</p><p>如果x与l之间距离近似于0，则新特征f→1；反之较远则f→0。</p><p>（SVM也可以不使用核函数，不使用又称为线性核函数，一般训练集特征非常多而实例非常少的时候可以采用）</p><p>下面是支持向量机的参数C（之前有）和$\delta$的影响：</p><p>1.$\delta$较大时，可能导致低方差，高偏差；</p><p>2.$\delta$较小时，可能导致低偏差，高方差。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第4篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="AndrewNg《MachineLearning》笔记" scheme="http://li-zhe.com/categories/AndrewNg%E3%80%8AMachineLearning%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="支持向量机" scheme="http://li-zhe.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(3)--神经网络(Neural Network)及模型改进</title>
    <link href="http://li-zhe.com/posts/1348010069/"/>
    <id>http://li-zhe.com/posts/1348010069/</id>
    <published>2019-12-18T01:44:34.000Z</published>
    <updated>2020-01-02T01:14:17.977Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第3篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>训练样本个数：</p><script type="math/tex; mode=display">m</script><p>每个样本包含一组输入和一组输出：</p><script type="math/tex; mode=display">x，y</script><p>表示神经网络层数Layer：</p><script type="math/tex; mode=display">L</script><p>表示每层的神经元个数：</p><script type="math/tex; mode=display">S_I</script><p>表示输出层神经元个数：</p><script type="math/tex; mode=display">S_l</script><p>表示最后一层中处理单元的个数：</p><script type="math/tex; mode=display">S_L</script><h3 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h3><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}[\sum_{i=1}^m\sum_{k=1}^ky_k^{(i)}log(h_\theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\theta(x^{(i)}))_k)]+\cfrac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{ji}^{(l)})^2</script><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>为了计算代价函数的偏导数$\cfrac{\partial}{\partial\theta_{ij}^{(l)}}$,先计算最后一层的误差，再一层一层反向求出各层误差，直到倒数第二层。</p><h3 id="（神经网络部分细节）亟待补充"><a href="#（神经网络部分细节）亟待补充" class="headerlink" title="* （神经网络部分细节）亟待补充"></a>* （神经网络部分细节）亟待补充</h3><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><p>1.参数的随机初始化</p><p>2.利用正向传播方法计算所有的$h_\theta(x)$</p><p>3.编写计算代价函数J的代码</p><p>4.利用反向传播方法计算所有偏导数</p><p>5.利用数值检验方法检验偏导数</p><p>6.利用优化算法来最小化代价函数</p><h2 id="模型应用建议（不只针对神经网络）"><a href="#模型应用建议（不只针对神经网络）" class="headerlink" title="模型应用建议（不只针对神经网络）"></a>模型应用建议（不只针对神经网络）</h2><p>当运用训练好了的模型预测未知数据时发现有较大误差，下一步？</p><p>1.减少特征的数量；</p><p>2.获取更多特征；</p><p>3.增加多项式特征（如$x^2，x_1·x_2$等）</p><p>4.减少or增加正则化程度$\lambda$</p><h3 id="对假设的评估"><a href="#对假设的评估" class="headerlink" title="对假设的评估"></a>对假设的评估</h3><p>通常，数据=70%训练集+30%测试集；获得模型后，可以：</p><p>1.线性回归：直接用测试集数据计算代价函数J；</p><p>2.逻辑回归：计算代价函数外，还可计算错误率；</p><h3 id="模型选择和交叉验证集"><a href="#模型选择和交叉验证集" class="headerlink" title="模型选择和交叉验证集"></a>模型选择和交叉验证集</h3><p>即，<strong>数据=60%训练集+20%交叉验证集+20%测试集</strong>；之后：</p><p>1.使用<strong>训练集</strong>训练处n个模型；</p><p>2.用n个模型分别对<strong>交叉验证集</strong>计算得出交叉验证误差（代价函数值）；</p><p>3.选取代价函数值最小的模型；</p><p>4.用3中选出的模型对<strong>测试集</strong>计算得出推广误差（代价函数值）。</p><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>偏差较大→欠拟合；方差较大→过拟合。</p><p>可以通过将训练集和交叉验证集的代价函数与<strong>多项式的次数</strong>绘制在同一张图表上来帮助分析，训练集代价函数肯定是随次数上升减小的，而交叉验证集是U形：</p><p>二者相近：偏差/欠拟合</p><p>交叉远大于训练：方差/过拟合</p><p>而<strong>正则化系数$\lambda$也</strong>有所贡献，训练集误差随着$\lambda$而增加，交叉验证集则随$\lambda$增加先减小后增加；因为$\lambda$增加使得$\theta$的贡献降低了：</p><p><strong>学习曲线</strong></p><p>将训练集误差和交叉验证集误差作为训练集实例数量（m）函数而绘制的图→说明高方差/过拟合情况下增加更多数据可能提升算法效果。</p><h3 id="建议Solution"><a href="#建议Solution" class="headerlink" title="建议Solution"></a>建议Solution</h3><p>1.获取更多的数据（训练实例）→高方差√ </p><p>2.减少特征数量→高方差√</p><p>3.增加特征数量→高偏差√</p><p>4.增加多项式特征→高偏差√</p><p>5.减少正则化程度$\lambda$→高偏差√</p><p>6.增加正则化程度$\lambda$→高方差√</p><h3 id="类偏斜"><a href="#类偏斜" class="headerlink" title="类偏斜"></a>类偏斜</h3><p>一个极端的例子，假设在用算法预测癌症是良性还是恶性，那么如果说训练集的样本中只有0.5%是恶性，其余全是良性；那么我们平白无故算法A预测所有肿瘤都是良性，误差为0.5%；而写出来的神经网络算法B训练后误差可能有1%，这个时候显然不能根据误差判断A比B算法好。</p><p><strong>查准率precision=TP/(TP+FP)</strong>，即所有预测有恶性肿瘤的病人中，实际有恶性肿瘤的百分比，越↑越好。</p><p><strong>查全率recall=TP/(TP+FN)</strong>，即所有实际有恶性肿瘤的病人中，被预测到的百分比，越↑越好。</p><p>如何权衡这两个指标呢？根据需求！</p><p>可以在不同阈值的情况下，将二者关系绘制成图表（x/y轴是二者）；一个帮助选择阈值的方法：<br><strong>F1值法</strong>：</p><script type="math/tex; mode=display">F1=\cfrac{2PR}{P+R}</script><p>选择使F1最高的阈值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第3篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="AndrewNg《MachineLearning》笔记" scheme="http://li-zhe.com/categories/AndrewNg%E3%80%8AMachineLearning%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://li-zhe.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模型改进" scheme="http://li-zhe.com/tags/%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(2)--逻辑回归(Logistic Regression)及正则化(Normalization)</title>
    <link href="http://li-zhe.com/posts/438233758/"/>
    <id>http://li-zhe.com/posts/438233758/</id>
    <published>2019-12-17T06:44:34.000Z</published>
    <updated>2020-01-02T01:13:53.425Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第2篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归(Logistic Regression)"></a>逻辑回归(Logistic Regression)</h2><p>引入模型，使得模型的输出变量范围始终在0和1之间，以便完成分类：</p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^TX)</script><p>X代表特征向量,g代表逻辑函数，此处为Sigmoid函数：</p><script type="math/tex; mode=display">g(z)=\cfrac{1}{1+e^{-z}}</script><p>故：</p><script type="math/tex; mode=display">h_\theta(x)=\cfrac{1}{1+e^{-\theta^TX}}</script><p>此时h的作用是对于给定的输入变量，输出变量=1的可能性，即：</p><script type="math/tex; mode=display">h_\theta(x)=P(y=1|x;\theta)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br></pre></td></tr></table></figure><p>我们预测的规则是h&gt;0.5则y=1,反之y=0；又因为g(z)的0.5分界在于z=0，所以$\theta^Tx?0$是判断的边界。故决策边界（曲线）方程即是：</p><script type="math/tex; mode=display">\theta^Tx=0</script><p>线性回归的代价函数：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>逻辑回归的代价函数：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)})-y^{(i)})</script><p>y=1时：</p><script type="math/tex; mode=display">Cost(h_\theta(x),y)=-log(h_\theta(x))</script><p>y=0时：</p><script type="math/tex; mode=display">Cost(h_\theta(x),y)=-log(1-h_\theta(x))</script><p>简化之：</p><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta,X,y)</span>:</span></span><br><span class="line">    theta=np.matrix(theta)</span><br><span class="line">    X=np.matrix(X)</span><br><span class="line">    y=np.matrix(y)</span><br><span class="line">    first=np.multiply(y,np.log(sigmoid(X*theta.T)))</span><br><span class="line">    second=np.multiply((<span class="number">1</span>-y),mnp.log(<span class="number">1</span>-sigmoid(X*theta.T)))</span><br><span class="line">    <span class="keyword">return</span> np.sum(first+second)/-(len(X))</span><br></pre></td></tr></table></figure><p>仍可以用梯度下降算法求使代价函数最小的参数：</p><script type="math/tex; mode=display">\theta_j=\theta_j-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x_j^{(i)})</script><p>易知代价函数会带来一个凸优化问题，即$J(\theta)$会是一个凸函数，并且没有局部最优值。</p><p>除了梯度下降之外，常用来令代价函数最小的算法：</p><ul><li>共而梯度Conjugate Gradient</li><li>局部优化法Broyden fletcher goldfarb shann,BFGS</li><li>有限内存局部优化法LBFGS</li></ul><h2 id="正则化-Normalization"><a href="#正则化-Normalization" class="headerlink" title="正则化(Normalization)"></a>正则化(Normalization)</h2><p>面对过拟合问题，一是想办法减少特征量，二便是正则化（保留所有特征但是减小参数的大小）。</p><p>过拟合来自于高次项（虚线过于扭曲），因而可以通过让高次项的系数接近于0来完成拟合。为此要在代价函数中为$\theta$添加惩罚。</p><p>此时的代价函数为：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]</script><p>$\lambda$即为正则化参数，过大会导致模型欠拟合，过小则导致所有$\theta$都趋于0。</p><h3 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h3><p>代价函数：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]</script><p>梯度下降（未对$\theta_0$正则化）：</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha[\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\cfrac{\lambda}{m}\theta_j]</script><p>正则回归(矩阵尺寸为(n+1)*(n+1))：</p><script type="math/tex; mode=display">\theta=(X^TX+\lambda[0 1 1 ... 1])^{-1}X^Ty</script><h3 id="正则化逻辑回归"><a href="#正则化逻辑回归" class="headerlink" title="正则化逻辑回归"></a>正则化逻辑回归</h3><p>代价函数：</p><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\cfrac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p>梯度下降（未对$\theta_0$正则化）：</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha[\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\cfrac{\lambda}{m}\theta_j]</script><p>看上去和线性回归一样，但是$h_\theta(x)=g(\theta^TX)$。</p><p>(P.S.文中偶尔出现代码使用的是Python3，因为是用JupyterNotebook写的笔记，所以顺手可以跑点Python代码测试一下😁)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第2篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="AndrewNg《MachineLearning》笔记" scheme="http://li-zhe.com/categories/AndrewNg%E3%80%8AMachineLearning%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://li-zhe.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="正则化" scheme="http://li-zhe.com/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(1)--单变量线性回归及多维特征情形</title>
    <link href="http://li-zhe.com/posts/1805300341/"/>
    <id>http://li-zhe.com/posts/1805300341/</id>
    <published>2019-12-16T01:44:34.000Z</published>
    <updated>2020-01-02T01:12:54.169Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第1篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>后续篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><p><strong>开篇，关于机器学习的定义：</strong></p><blockquote><p>TomMitchell:<br>一个程序从经验E中学习，解决任务T，达到性能度量值P；当且仅当有了经验E后，经P评判，程序处理T时的性能有所提高。</p></blockquote><h2 id="从单变量的线性回归开始"><a href="#从单变量的线性回归开始" class="headerlink" title="从单变量的线性回归开始"></a>从单变量的线性回归开始</h2><p>特征的数量：<script type="math/tex">1</script><br>训练集中实例的数量：<script type="math/tex">m</script><br>特征/输入变量：<script type="math/tex">x</script><br>目标/输出变量：<script type="math/tex">y</script><br>训练集中的实例：<script type="math/tex">(x,y)</script><br>训练集中第i个实例：<script type="math/tex">(x^{(i)},y^{(i)})</script><br>函数/假设：<script type="math/tex">h</script><br>单变量线性回归问题：<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x</script><br>代价函数（建模误差）：<script type="math/tex">J(\theta_0,\theta_1)=\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><br>我们的目标是建立模型h，使得代价函数J最小，即：<script type="math/tex">\underbrace{minimize}_{\theta_0,\theta_1}J(\theta_0,\theta_1)</script></p><h3 id="批量梯度下降-同时更新2个theta-："><a href="#批量梯度下降-同时更新2个theta-：" class="headerlink" title="批量梯度下降(同时更新2个theta)："></a>批量梯度下降(同时更新2个theta)：</h3><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)</script><p>（更新theta的原理在于后面的导数项，通过斜率正负确保随着theta的增大或减小，J(theta）一定随之减小，同时因为J(theta)的斜率变化，theta的变化也随着J的减小而减小，如在接近局部最低点时导数接近0，此时梯度下降法会自动采取更小的幅度）。</p><p>所以梯度下降的实现关键在于求出代价函数J的导数：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\cfrac{\partial}{\partial\theta_j}\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>j=0：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})</script><p>j=1：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\cfrac{1}{2m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x^{(i)})</script><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>通过求解方程来找出使J最小的参数(不可逆矩阵不可用)：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_j)=0</script><p>假设训练集的特征矩阵为X，训练集的结果为向量y，则：</p><script type="math/tex; mode=display">\theta=(X^TX)^{(-1)}X^Ty</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta=np.linalg.inv(X.T@X)@X.T@y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><h2 id="多维特征情形"><a href="#多维特征情形" class="headerlink" title="多维特征情形"></a>多维特征情形</h2><h3 id="通用情形"><a href="#通用情形" class="headerlink" title="通用情形"></a>通用情形</h3><p>特征的数量：<script type="math/tex">n</script><br>训练集中的实例成为行向量：<script type="math/tex">x^{(i)}</script><br>第i个训练实例的第j个特征：<script type="math/tex">x_j^{(i)}</script><br>X是类似m×n的矩阵，m是实例个数，n是特征个数</p><p>多变量的假设：<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script><br>为了简化公式，引入$x_0=1$：</p><script type="math/tex; mode=display">h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script><p>即：<script type="math/tex">h_\theta(x)=\theta^T(X)</script><br>我们的目标和单变量情形下一样，只不过theta更多：</p><script type="math/tex; mode=display">\theta_0=\theta_0-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x_0^{(i)})</script><script type="math/tex; mode=display">\theta_1=\theta_1-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x_1^{(i)})</script><script type="math/tex; mode=display">...</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X,y,theta)</span>:</span></span><br><span class="line">    inner=np.power(((X*theta.T)-y),<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner)/(<span class="number">2</span>*len(X))</span><br><span class="line"><span class="comment">#len(X)代表m而不是n，注意！</span></span><br></pre></td></tr></table></figure><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h4 id="特征放缩"><a href="#特征放缩" class="headerlink" title="特征放缩"></a>特征放缩</h4><p>当两个特征值范围差的很远，如0-5和0-2000时，代价函数的等高线图能看出图像很扁，梯度下降也需要非常多次迭代才能收敛，方法是尝试将所有特征的尺度都缩放到-1到1之间：</p><script type="math/tex; mode=display">x_n=\cfrac{x_n-\mu_n}{s_n}</script><p>(平均值$\mu_n$，标准差$s_n$)</p><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p>梯度下降算法的迭代受到学习率alpha的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如过大，则每次迭代可能不会减小代价函数，越过局部最小值而导致无法收敛。<br>建议的学习率：</p><script type="math/tex; mode=display">\alpha=0.01,0.03,0.1,0.3,1,3,10</script><p>(P.S.文中偶尔出现代码使用的是Python3，因为是用JupyterNotebook写的笔记，所以顺手可以跑点Python代码测试一下😁)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第1篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="AndrewNg《MachineLearning》笔记" scheme="http://li-zhe.com/categories/AndrewNg%E3%80%8AMachineLearning%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://li-zhe.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="梯度下降" scheme="http://li-zhe.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://li-zhe.com/posts/1243066710/"/>
    <id>http://li-zhe.com/posts/1243066710/</id>
    <published>2019-12-13T03:00:20.236Z</published>
    <updated>2019-12-17T02:25:01.672Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎来到我的二代站点，一代见左侧<strong>Link To</strong>处“Previous Site”。</p><p>不出意外的话它将在2020年1月11日关闭，原因是<del>负担不起10块一个月的服务器费用了(并不)</del>比较复杂的。</p><p>届时可能会把域名迁移过来(<del>虽然喜新厌旧的我已经打算买新的了</del>)，反正看的人也不多，还是用git省心一点嗷。</p><p>先这样吧，等我慢慢把评论功能和题图啥的整出来再开始写文章好了。</p><p>偷偷测试一下emoji：💉💦🐂🍺👌😂</p><p>( •̀ ω •́ )y</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;欢迎来到我的二代站点，一代见左侧&lt;strong&gt;Link To&lt;/strong&gt;处“Previous Site”。&lt;/p&gt;
&lt;p&gt;不出意外的话它将在2020年1月11日关闭，原因是&lt;del&gt;负担不起10块一个月的服务器费用了(并不)&lt;/del&gt;比较复杂的。&lt;/p&gt;
&lt;p&gt;届
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
