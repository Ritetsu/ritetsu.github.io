<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://li-zhe.com/"/>
  <updated>2020-10-08T07:39:46.751Z</updated>
  <id>http://li-zhe.com/</id>
  
  <author>
    <name>LiZhe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>国庆假期的潦草总结</title>
    <link href="http://li-zhe.com/posts/20201008/"/>
    <id>http://li-zhe.com/posts/20201008/</id>
    <published>2020-10-08T07:55:54.000Z</published>
    <updated>2020-10-08T07:39:46.751Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前"><a href="#前" class="headerlink" title="前"></a>前</h3><p>假期，唯独在开始之前的那么一两天最有意思。</p><p>尤其是长假开启前的当天晚上，夜幕落下时，仿佛能隔绝一切现实世界里的烦恼。直观地感受就是：为所欲为、无所不能、充满希望。</p><p>（P.S. 背景图是我夜里在洗衣房拍的，颇有灯红酒绿的感觉）</p><p>一旦它开始，头几天同样充满着乐趣。</p><p>你能够从容地去做一些计划里的事情，比如呼朋引伴出去吃东西去唱歌去打球，比如去健身房把自己练得第二天下不了楼梯，比如一个人在秋高气爽时骑车到处逛逛……哪怕只是突然地灵光一现想到什么事情，也可以立刻去做，相当地逍遥自在。</p><p>保研结束后在北京的那段时间里，我一直想过这种日子。但因为当时选导师的焦虑作祟，只在生日那天稍微放空了一下。</p><p>独自去景山公园的北京中轴线上，花一整个下午的时间看故宫和日落，想到这竟然是我某种意义上离开北京前做的最后一件事，实在是颇为浪漫。</p><h3 id="中"><a href="#中" class="headerlink" title="中"></a>中</h3><p>说回假期，前几日的欢乐掠过，眨眼便到了中后期。一是游玩的疲乏袭来，二是工作日眼看就在逼近，人的心态不免发生变化。</p><p>八天假期的第六天，和两天假期的第一天，着实是截然不同的体验。</p><p>也许是我过于「未雨绸缪」，往往在假期余额还算充足的时候，我的思绪就会飘向正常的生活。</p><p>一边提前收心准备上课和科研，一边又意犹未尽地想最后再出去玩一趟。虽然有点给自己找不痛快的意思，但我想也正是这样的举动才让我从假期到工作日的过渡更加自然，至少不会有戛然而止的失落感。</p><p>吃喝玩乐的细节和朋友圈里令人眼花缭乱的图片大同小异，不值得详叙，也没拍下什么特别想分享的照片，姑且这样略过。</p><p>国庆这几天过得确是飞快，有那么几天直接把整个上午都交待在了被窝里，又有那么几个下午在实验室装主机、软件和环境。剩下的时间，安排上了一些大大小小的活动，和一些自由发呆的时光，也还算充实。</p><h3 id="后"><a href="#后" class="headerlink" title="后"></a>后</h3><p>下一个长假就是过年了，因为隔得太远了暂时还没有什么期待。倒是眼下就要到来的课程、轮转和课题，让人充满了对未知的新奇感。</p><p>此外，在假期里，用新的大屏幕补完了《教父》三部曲，感想挺多，但看到网上清一色地「男人必看的经典」这种过于浅显粗俗的评论，突然又失去了把它拿出来说几句的欲望。与此稍微有些相关的，周杰伦的以父之名很好听，也很好唱，就是有点废舌头。</p><p>最后，再插个题外话，很久没上自己的网站，再上时惊觉阅读量有了肉眼可见的增长。虽然从绝对数量上来说并不多，但足以证明真的有不少人点进来看过。</p><p>我只在空间里发布过这个地址，所以来的至少是我认识的人。</p><p>不知道你是谁，但谢谢你，愿意看我发牢骚。</p><p>（发牢骚这里本来想用无病呻吟，但我寻思就凭我写的这些东西应当还不至于，而且因为某些原因我突然对这个词有点ptsd……）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;前&quot;&gt;&lt;a href=&quot;#前&quot; class=&quot;headerlink&quot; title=&quot;前&quot;&gt;&lt;/a&gt;前&lt;/h3&gt;&lt;p&gt;假期，唯独在开始之前的那么一两天最有意思。&lt;/p&gt;
&lt;p&gt;尤其是长假开启前的当天晚上，夜幕落下时，仿佛能隔绝一切现实世界里的烦恼。直观地感受就是：为
      
    
    </summary>
    
    
      <category term="个人随想" scheme="http://li-zhe.com/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="个人随想" scheme="http://li-zhe.com/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>给大学生活画上不圆满的句号</title>
    <link href="http://li-zhe.com/posts/20200625/"/>
    <id>http://li-zhe.com/posts/20200625/</id>
    <published>2020-06-25T08:55:54.000Z</published>
    <updated>2020-08-05T12:28:57.952Z</updated>
    
    <content type="html"><![CDATA[<h3 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h3><p>得益于转行做了生信，我的毕业论文并没有受到「因为疫情无法返校做实验」这一事件的影响。虽然在家里的学习工作效率比不上在实验室，但还是顺利完成了毕业论文。一切敲定后整理格式，才发现自己洋洋洒洒竟写了两万多字。</p><p>把稿子交给老板，忐忑地等待回复、准备修改，却得到了「综述写得挺好，像博士论文」的谬赞。返回的稿子一字未改，当然，我自知不是真的有多好，只是相比于细细读这两万多字的本科毕业论文然后写满批注，大致地扫几眼再给一些夸奖和鼓励或许是对二者都更好的选择。</p><h3 id="答辩"><a href="#答辩" class="headerlink" title="答辩"></a>答辩</h3><p>答辩过程无惊无险，没有什么印象特别深刻的事情，只记得有些同学的 PPT 做得是真的简陋，好吧，其实是「丑」。</p><p>再次感慨「Listening and Presentation」是一门多么有用的课程。</p><h3 id="返校"><a href="#返校" class="headerlink" title="返校"></a>返校</h3><p>当得到通知，返校时间定在 6 月 17 日的时候，我感觉一切都好起来了。</p><p>踏进的 430 ，和室友并肩走进充满「食物味道」的颐二，在清真窗口点一份「一半加蘑菇酱一半加咖喱酱」的鸡排饭，吃完直接上床把午觉睡得天昏地暗，下午也许一起出去吃顿好的，晚上凑在谁谁的屏幕后面一起看电影或者玩恐怖游戏。</p><p>一切都是那么熟悉，那么令人怀念。</p><p>不过突然反复的疫情让这一切都只能存在于回忆里了。</p><p>说实话我真的很想回去，也许是那几天因为返校的事情左右横跳，导致情绪不太稳定，我难得地在朋友圈抒发了对学校返校策略很失望的负面情绪。</p><p>很不幸，虽然得到了同学们的点赞，但被学院的领导在评论里「定了罪」，说是希望在学院呆了四年不要只教会了我「腹诽」。看到这条评论我很诧异，看到它来自于谁我更加诧异。本想据理力争的我，权衡了利弊之后选择了删除这条朋友圈。于我而言，这是一段非常讽刺的经历，当然讽刺的不是我，而是「」。</p><p>此外，期间一些同的表态也让我意识到，什么叫「精致的利己主义者」。也许成绩很好，甚至平常上课表现乖巧，和同学相处融洽，可一旦涉及到自己的利益，便立刻急切地跳出来试图引导舆论风向，吃相之难看令人有些瞠目。如果有人看到这里感觉不适，请务必不要对号入座，请务必直接右上角关闭网页，我不针对任何人，也不想和任何人对线。</p><h3 id="典礼"><a href="#典礼" class="headerlink" title="典礼"></a>典礼</h3><p>事前我给室友打预防针，说不要期待太多，能有个线上拨穗仪式就很不错了。室友倒是反过来信誓旦旦地告诉我「学校肯定会允许我们以后哪年再回来参加毕业典礼的」。</p><p>事实上，看完线上毕业典礼后，根据弹幕和群里热烈的讨论，可以发现几乎所有毕业生的心中都只有一句「就这？」。当然，这位对我校尚怀有期待的室友受到的伤害，比我这种没抱着什么期望去看的要更大。</p><p>后来学院做的视频和毕业直播，都比学校有诚意得多。毕业小礼物也让我这种冷漠的人有了一些些动容。</p><p>「人文关怀」真的值得太多人学习。</p><h3 id="行李"><a href="#行李" class="headerlink" title="行李"></a>行李</h3><p>连清理宿舍都无法自己亲自动手，不得不远程视频拜托志愿者打包。透过模糊的视频看到寝室时，我几乎记不得我桌上还有些什么，各种盒子、箱子又有多少是属于我的。</p><p>总之感谢收拾寝室的志愿者们，其中甚至不乏我院的院长。虽然「杰青打包组」更像是个噱头，但几位杰青辛苦劳动帮同学们打包行李付出的汗水却是实实在在的。无论如何，在这里再次感谢各位。</p><p>打包时，对面的志愿者一直在问我「这个要吗？那个要吗？」，而我只能卑微地回答「要的，要的……」。</p><p>其实我巴不得能全部寄回来，因为我想亲手好好收拾这四年里的收获。但考虑到给正在打包的，和后续要搬箱子的志愿者们减减负，我主动放弃了很多东西。</p><h3 id="尾声"><a href="#尾声" class="headerlink" title="尾声"></a>尾声</h3><p>随着行李被家人签收，毕业证学位证也由学院寄出，我终于感觉到与母校之间有什么联系断开了，我们真的就这样遥遥告别了。</p><p>校园里再也没有「实物」什么是属于我的，我的寝室、床铺将归属于下一个入住的学弟；一些没带走的东西也会被当成垃圾杂物清理掉。</p><p>不过我虽然带走了自己留下的痕迹，却带不走校园里承载着的时光和故事。有很多朋友都留在了北京深造，相信我将来也一定会因为现在预想不到的原因再回到北京，那时，我会郑重其事地返校，弥补如今留有遗憾的一切。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;论文&quot;&gt;&lt;a href=&quot;#论文&quot; class=&quot;headerlink&quot; title=&quot;论文&quot;&gt;&lt;/a&gt;论文&lt;/h3&gt;&lt;p&gt;得益于转行做了生信，我的毕业论文并没有受到「因为疫情无法返校做实验」这一事件的影响。虽然在家里的学习工作效率比不上在实验室，但还是顺利完成了
      
    
    </summary>
    
    
      <category term="个人随想" scheme="http://li-zhe.com/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="个人随想" scheme="http://li-zhe.com/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>关于毕业论文的小吐槽</title>
    <link href="http://li-zhe.com/posts/20200422/"/>
    <id>http://li-zhe.com/posts/20200422/</id>
    <published>2020-04-22T15:59:59.000Z</published>
    <updated>2020-08-05T13:00:21.552Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ⅰ-要求"><a href="#Ⅰ-要求" class="headerlink" title="Ⅰ 要求"></a>Ⅰ 要求</h2><p>在学校的一次大会上，主持人突然说：下面认为本科生毕设应该改综述的请坐到左边，认为应该按照原规范进行的请坐到右边。大部分人坐到了左边，少数人坐到右边。</p><p>只有一个人还坐在中间不动。</p><p>主持人：这位朋友，你认为本科生毕设应该怎么进行下去？</p><p>那人回答：我一开始告诉他们重点考察综述，但我同时希望按“原计划”“原规范”进行。</p><p>主持人慌忙说：原来是__学院，请您赶快坐到主席台上来。</p><h2 id="Ⅱ-进度"><a href="#Ⅱ-进度" class="headerlink" title="Ⅱ 进度"></a>Ⅱ 进度</h2><p>本科生即将毕业，学校令画家创作一幅名为《毕业献礼》的画作，展现老师们如何帮助同学顺利完成毕业设计。</p><p>画完成后，有人前来验收：画面上，X学院的实验以代码的形式在线上很好地完成了；Y学院把论文改成了综述，大家已经开始答辩；Z学院的同学已经完成了查重，开始了紧张刺激的降重行动。</p><p>“可是，怎么没看到__学院？”验收的人问道。</p><p>“__学院在和他们的学生开会”画家说。</p><p>“开会？是毕业欢送会吗？”</p><p>“不，是说明会，讨论论文到底该写什么。”</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Ⅰ-要求&quot;&gt;&lt;a href=&quot;#Ⅰ-要求&quot; class=&quot;headerlink&quot; title=&quot;Ⅰ 要求&quot;&gt;&lt;/a&gt;Ⅰ 要求&lt;/h2&gt;&lt;p&gt;在学校的一次大会上，主持人突然说：下面认为本科生毕设应该改综述的请坐到左边，认为应该按照原规范进行的请坐到右边。大部分人坐到
      
    
    </summary>
    
    
      <category term="时事评论" scheme="http://li-zhe.com/categories/%E6%97%B6%E4%BA%8B%E8%AF%84%E8%AE%BA/"/>
    
    
      <category term="时事评论" scheme="http://li-zhe.com/tags/%E6%97%B6%E4%BA%8B%E8%AF%84%E8%AE%BA/"/>
    
      <category term="苏联笑话" scheme="http://li-zhe.com/tags/%E8%8B%8F%E8%81%94%E7%AC%91%E8%AF%9D/"/>
    
  </entry>
  
  <entry>
    <title>在家里也能用的基因组浏览器</title>
    <link href="http://li-zhe.com/posts/20200301/"/>
    <id>http://li-zhe.com/posts/20200301/</id>
    <published>2020-03-01T15:59:59.000Z</published>
    <updated>2020-07-25T13:39:41.678Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>注：我测试时的环境是 <strong>未安装其他版本的Java和X11组件的服务器</strong>，其他情况请按需修改对应步骤。</p></blockquote><h1 id="1-前期准备"><a href="#1-前期准备" class="headerlink" title="1.前期准备"></a>1.前期准备</h1><h2 id="1-1-下载Linux版本的IGV"><a href="#1-1-下载Linux版本的IGV" class="headerlink" title="1.1 下载Linux版本的IGV"></a>1.1 下载Linux版本的IGV</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://data.broadinstitute.org/igv/projects/downloads/2.8/IGV_Linux_2.8.0.zip</span><br><span class="line">unzip IGV_Linux_2.8.0.zip</span><br><span class="line"><span class="built_in">cd</span> IGV_Linux_2.8.0        </span><br><span class="line">cat readme.txt      <span class="comment">#确认所需环境（目前版本需要Java11）</span></span><br></pre></td></tr></table></figure><h2 id="1-2-配置Java11运行环境"><a href="#1-2-配置Java11运行环境" class="headerlink" title="1.2 配置Java11运行环境"></a>1.2 配置Java11运行环境</h2><h3 id="root用户"><a href="#root用户" class="headerlink" title="root用户"></a>root用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum search java-11-openjdk              <span class="comment">#找到对应的development版本</span></span><br><span class="line">yum install java-11-openjdk-devel.x86_64</span><br><span class="line">java -version                           <span class="comment">#测试</span></span><br></pre></td></tr></table></figure><h3 id="普通用户"><a href="#普通用户" class="headerlink" title="普通用户"></a>普通用户</h3><p>前往官网/其他地址下载安装包：<br><a href="https://openjdk.java.net/install/index.html" target="_blank" rel="noopener">https://openjdk.java.net/install/index.html</a></p><p><a href="https://download.java.net/openjdk/jdk11/ri/openjdk-11+28_linux-x64_bin.tar.gz" target="_blank" rel="noopener">https://download.java.net/openjdk/jdk11/ri/openjdk-11+28_linux-x64_bin.tar.gz</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">wget https://download.java.net/openjdk/jdk11/ri/openjdk-11+28_linux-x64_bin.tar.gz</span><br><span class="line">tar -zxvf openjdk-11+28_linux-x64_bin.tar.gz</span><br><span class="line">vi ~/.bash_profile</span><br><span class="line"><span class="comment">#填入如下内容(自行修改JAVA所在路径)</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/your_path/jdk-11</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br><span class="line"><span class="built_in">source</span> ~/.bash_profile</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure></p><h2 id="1-3-启用服务器X11转发"><a href="#1-3-启用服务器X11转发" class="headerlink" title="1.3 启用服务器X11转发"></a>1.3 启用服务器X11转发</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p>修改以下内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#AllowAgentForwarding yes</span><br><span class="line">AllowTcpForwarding yes</span><br><span class="line">#GatewayPorts no</span><br><span class="line">X11Forwarding yes</span><br><span class="line">X11DisplayOffset 10</span><br><span class="line">X11UseLocalhost no   #这里可以保持默认不修改</span><br><span class="line">#PermitTTY yes</span><br><span class="line">#PrintMotd yes</span><br><span class="line">#PrintLastLog yes</span><br><span class="line">#TCPKeepAlive yes</span><br></pre></td></tr></table></figure><br>安装X11组件包及字库：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y xorg-x11-xauth           <span class="comment">#安装x11组件包        </span></span><br><span class="line">yum install -y wqy-zenhei-fonts*        <span class="comment">#安装中文字库</span></span><br></pre></td></tr></table></figure></p><h1 id="2-运行IGV"><a href="#2-运行IGV" class="headerlink" title="2.运行IGV"></a>2.运行IGV</h1><h2 id="2-1-使用Xshell6-Xmanager6"><a href="#2-1-使用Xshell6-Xmanager6" class="headerlink" title="2.1 使用Xshell6+Xmanager6"></a>2.1 使用Xshell6+Xmanager6</h2><p>①下载Xshell及Xmanager，在Xshell中更改设置：</p><p>连接→SSH→隧道→ 勾选 转发X11连接到Xmanager；</p><p>②打开Xmanager，通过Xshell正常连接，进入IGV文件夹内：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh igv.sh</span><br></pre></td></tr></table></figure></p><h2 id="2-2-使用PuTTY-Xming"><a href="#2-2-使用PuTTY-Xming" class="headerlink" title="2.2 使用PuTTY+Xming"></a>2.2 使用PuTTY+Xming</h2><p>①下载Xming并打开（不需要设置）；</p><p>②下载PuTTY，更改设置：</p><p>Connection → SSH → X11 → 勾选Enable X11 forwarding；</p><p>同上，正常连接使用。</p><h2 id="2-3-使用MobaXterm-推荐"><a href="#2-3-使用MobaXterm-推荐" class="headerlink" title="2.3 使用MobaXterm(推荐)"></a>2.3 使用MobaXterm(推荐)</h2><p>①下载MobaXterm；</p><p>②同上，正常连接使用。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;注：我测试时的环境是 &lt;strong&gt;未安装其他版本的Java和X11组件的服务器&lt;/strong&gt;，其他情况请按需修改对应步骤。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;1-前期准备&quot;&gt;&lt;a href=&quot;#1-前期准备&quot; class=
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="IGV" scheme="http://li-zhe.com/tags/IGV/"/>
    
      <category term="Linux" scheme="http://li-zhe.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>《阿房宫赋》</title>
    <link href="http://li-zhe.com/posts/20200209/"/>
    <id>http://li-zhe.com/posts/20200209/</id>
    <published>2020-02-09T08:59:59.000Z</published>
    <updated>2020-08-05T13:00:05.088Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>2020年的开年是如此不平凡。闲在家里回忆了不少往事，监督妹妹背书的时候突然想起来自己以前背过的诗词古文，一句两句地这么一背，居然一字不差。</p></blockquote><h1 id="《阿房宫赋》"><a href="#《阿房宫赋》" class="headerlink" title="《阿房宫赋》"></a>《阿房宫赋》</h1><p><strong>唐·杜牧</strong></p><p>六王毕，四海一。蜀山兀，阿房出。覆压三百余里，隔离天日。骊山北构而西折，直走咸阳。二川溶溶，流入宫墙。五步一楼，十步一阁；廊腰缦回，檐牙高啄；各抱地势，钩心斗角。盘盘焉，囷囷焉，蜂房水涡，矗不知其几千万落。长桥卧波，未云何龙？复道行空，不霁何虹？高低冥迷，不知西东。歌台暖响，春光融融；舞殿冷袖，风雨凄凄。一日之内，一宫之间，而气候不齐。</p><p>妃嫔媵嫱，王子皇孙，辞楼下殿，辇来于秦。朝歌夜弦，为秦宫人。明星荧荧，开妆镜也；绿云扰扰，梳晓鬟也；渭流涨腻，弃脂水也；烟斜雾横，焚椒兰也。雷霆乍惊，宫车过也；辘辘远听，杳不知其所之也。一肌一容，尽态极妍，缦立远视，而望幸焉。有不见者，三十六年。</p><p>燕赵之收藏，韩魏之经营，齐楚之精英，几世几年，剽掠其人，倚叠如山。一旦不能有，输来其间。鼎铛玉石，金块珠砾，弃掷逦迤，秦人视之，亦不甚惜。</p><p>嗟乎！一人之心，千万人之心也。秦爱纷奢，人亦念其家。奈何取之尽锱铢，用之如泥沙？使负栋之柱，多于南亩之农夫；架梁之椽，多于机上之工女；钉头磷磷，多于在庾之粟粒；瓦缝参差，多于周身之帛缕；直栏横槛，多于九土之城郭；管弦呕哑，多于市人之言语。使天下之人，不敢言而敢怒。独夫之心，日益骄固。戍卒叫，函谷举，楚人一炬，可怜焦土！</p><p>呜呼！灭六国者六国也，非秦也；族秦者秦也，非天下也。嗟乎！使六国各爱其人，则足以拒秦；使秦复爱六国之人，则递三世可至万世而为君，谁得而族灭也？秦人不暇自哀，而后人哀之；后人哀之而不鉴之，亦使后人而复哀后人也。</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>这段时间，不少人每看见一个突发事件的新闻就要发一句“2020年是什么神仙开局”，包括但不限于球星去世/地震/枪击案/脱欧等等。事实上真正值得提的事情只有一件，它如此重要、影响如此深远（我都不敢说是不是“远”，至少是很严重）以至于引得这么多人无病呻吟。</p><p>然而，1月19号时我还和伙伴们结伴去外地玩，甚至住了一天才回来。现在想起来都有些后怕，这一趟欢乐的行程里不知道冒了多少风险。说起来可笑，我们返程的路上已经在火车站看到有人戴口罩了，可彼时大家还在讨论“造谣”。回家后，事态飞速发展（当然我相信不是这短短一两天发展起来的，只是……），一两天之后就已经到了无法出门的程度。</p><p>闲在家里也看了不少新闻，时而热血沸腾，不过更多时候都是愤慨和叹息。反倒是偶尔静下心看看书的时候，能体会到很多小时候不明白的道理。就像文献读不通过一段时间再看又有新理解一样。希望今后让人失望的事情会越来越少，后人哀之亦鉴之吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;2020年的开年是如此不平凡。闲在家里回忆了不少往事，监督妹妹背书的时候突然想起来自己以前背过的诗词古文，一句两句地这么一背，居然一字不差。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;《阿房宫赋》&quot;&gt;&lt;a href=&quot;#《阿房宫赋》&quot; c
      
    
    </summary>
    
    
      <category term="时事评论" scheme="http://li-zhe.com/categories/%E6%97%B6%E4%BA%8B%E8%AF%84%E8%AE%BA/"/>
    
    
      <category term="时事评论" scheme="http://li-zhe.com/tags/%E6%97%B6%E4%BA%8B%E8%AF%84%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>2019年的结束&amp;2020年的开始</title>
    <link href="http://li-zhe.com/posts/20191231/"/>
    <id>http://li-zhe.com/posts/20191231/</id>
    <published>2019-12-31T15:59:59.000Z</published>
    <updated>2020-08-05T13:00:36.885Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>入夜莫名焦虑开始怀(自)旧(黑)。进入大学以来我好像一直在忙各种各样的事情，习惯了独来独往之后才发现更喜欢那种身边都是熟悉的人的感觉。有时候真想回到初三把所有人的同学录都好好地再写一遍，而不是草草地“少年珍重！”四个大字结束，那真的是我最不走心，也最开心的三年了。高中的日子快得来不及回想，但唯独那些人的笑容我竟也还记得清楚。<br>2018年，我一定要过得更深刻一些。</p></blockquote><p>以上是2017年1月8日我写下的总结，回首望去，原来很早我就意识到了常伴于身的焦虑，但却直到现在才开始正视它们。</p><p>太惭愧了。</p><h1 id="2019"><a href="#2019" class="headerlink" title="2019"></a>2019</h1><h2 id="1-N2测试：有惊无险；"><a href="#1-N2测试：有惊无险；" class="headerlink" title="1.N2测试：有惊无险；"></a>1.N2测试：有惊无险；</h2><p>尽管听听力时满头问号的感觉记忆犹新，但现在我有底气对每一个想考N2的人轻描淡写地说“N2不难”了；也可以（光明正大地）对知乎上那些回答“如何x天内零基础考过N2”然后疯狂卖资料或者给公众号/app打广告的答主们嗤之以鼻了。</p><p>（当然不管我考没考过，我都对这种假装好意分享实则浪费小白时间的人感到不屑）</p><p>（对于那种从来不主动学日语，但是到处搜集“资料”“秘籍”并自认行家的蠢人我也感到遗憾）</p><p>写到这不禁想起之前在B站看到的“总有人看了几集动漫就以为自己会日语了”的视频，我他娘的还真有这种朋友，成天なにそっかなるほど挂在嘴边，真跟他说起日语来又只能装死。不过毕竟是兄弟，也就图一乐，要是不熟的人操着塑料日语来跟我装，我是<del>（直接突突了）</del>根本不会理睬的。</p><p>可惜，N2考前的那段时光恐怕就是我迄今的日语水平巅峰了，毕竟双学位结课后基本上没什么机会<del>（炫耀）</del>用到日语。</p><p>赶紧把去日本旅游提上日程，争取好好锻炼一下口语。</p><h2 id="2-日语双学位：后知后觉；"><a href="#2-日语双学位：后知后觉；" class="headerlink" title="2. 日语双学位：后知后觉；"></a>2. 日语双学位：后知后觉；</h2><p>双学位的课程顺利结束了，而我一如既往地在彻底结束后一段时间才真切地意识到这件事情。</p><p>当周六早上顺着生物钟醒来却发现没有课可以上，当在食堂里和很久不见的只有在双学位课上才有机会多看几眼的同学相遇，当瞥见书桌上的日语教材和笔记却突然回想不起这是哪一门课上留下的记录时，我才突然反应过来，和一群在某些亚文化圈有高度重合的兴趣的人们就此别过了。</p><p>出于胆怯<del>（本来想写礼貌，考虑到在批判自己，还是真切一点好）</del>，没能和其中感兴趣的几个人建立更深的联系，以后只能做朋友圈的点赞之交，属实是今年的一大遗憾。</p><h2 id="3-自学生信：重大抉择；"><a href="#3-自学生信：重大抉择；" class="headerlink" title="3. 自学生信：重大抉择；"></a>3. 自学生信：重大抉择；</h2><p>不得不说学院把《生物信息学》排在大三下学期是真的有点坑。</p><p>为了不在夏令营时只能和老师谈情怀和说“我不会但我可以学”这种p话，我在寒假回家后紧急开始了自学。毫无基础的我一开始学得一头雾水，一个隐马尔可夫模型看了几天都看不明白，导致寒假一度在家学到自闭。</p><p>开学后试图去做科研训练，但仅凭一个寒假的入门性学习还是没能跟上师兄师姐们嘴里飘出的深度学习和卷积神经网络。</p><p>受到打击的我在最后一个学期抽出时间学了很多东西，包括生信/R/Python/机器学习甚至C语言。下定决心花时间去学算是我做出的重大抉择了，当然世界线的变动可能在几个月之前的某堂课上，那又是另一个故事了。</p><h2 id="4-夏令营：一言难蔽；"><a href="#4-夏令营：一言难蔽；" class="headerlink" title="4. 夏令营：一言难蔽；"></a>4. 夏令营：一言难蔽；</h2><p>为什么不出国留学，为什么没去TOP2，又为什么在高校和中科院之间选择了后者，以及各种夏令营期间富有冲击力的遭遇。</p><p>这其中的每一件事细说起来都能写出几千字的长文；考虑到最终的结果还远没有完全确定，这一段就先作罢。</p><h2 id="5-人际交往：一塌糊涂；"><a href="#5-人际交往：一塌糊涂；" class="headerlink" title="5. 人际交往：一塌糊涂；"></a>5. 人际交往：一塌糊涂；</h2><p>我没有毁掉任何已有的人际关系。我相信我和朋友们的友谊依旧坚如磐石。</p><p>一塌糊涂的是我自认这方面几乎没有成长，没有主动地去结识感兴趣的人。换而言之，我在不那么熟悉的人面前还是放不开，感觉像是一种极度扭曲的“偶像光环”。当然我不是偶像，也深知没有人会特别关注到我。但我心里总觉得与其留下不好的奇怪的印象，倒不如不去过多地展现自己。这种想法在形式上的体现是日常生活中的独行者，朋友圈里不留痕迹的过客；造成的结果是主动斩断了自己为数不多的社交。</p><p>当我数次想出去吃东西或者去哪玩玩逛逛，却只能想出几个屈指可数的人选，而他们偏偏还不想去时，我觉得2020年我需要在这方面做出一些改变。</p><h2 id="6-异性关系：注孤生；"><a href="#6-异性关系：注孤生；" class="headerlink" title="6. 异性关系：注孤生；"></a>6. 异性关系：注孤生；</h2><p>大学已经过去了三年，愣是没有什么故事可说。</p><p>被表白过一次，但是回想起事情发生的突然和人设的巨大反差，我总怀疑这件事并没有发生过。</p><p>一边有所期待有所企图，一边却又不敢主动，说注孤生大概没有毛病。</p><h1 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h1><p>喜闻乐见的立FLAG环节。</p><p>本来立了几个“几点前睡觉，睡前不玩手机，xx考多少分”<strong>之外</strong>的FLAG，但是仅仅一周后再看，我就觉得很蠢。</p><p>所以就暂时没有FLAG了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;入夜莫名焦虑开始怀(自)旧(黑)。进入大学以来我好像一直在忙各种各样的事情，习惯了独来独往之后才发现更喜欢那种身边都是熟悉的人的感觉。有时候真想回到初三把所有人的同学录都好好地再写一遍，而不是草草地“少年珍重！”四个大字结束，那真的是我最不走心，也
      
    
    </summary>
    
    
      <category term="生活总结" scheme="http://li-zhe.com/categories/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="生活总结" scheme="http://li-zhe.com/tags/%E7%94%9F%E6%B4%BB%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(6)推荐系统(Recommender Systems)及大规模机器学习</title>
    <link href="http://li-zhe.com/posts/20191221/"/>
    <id>http://li-zhe.com/posts/20191221/</id>
    <published>2019-12-21T04:17:34.000Z</published>
    <updated>2020-07-25T13:38:36.562Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第6篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><p>想想网易云音乐是如何进行每日推荐？这就是推荐系统。</p><h3 id="基于内容的推荐系统"><a href="#基于内容的推荐系统" class="headerlink" title="基于内容的推荐系统"></a>基于内容的推荐系统</h3><p>假设有不同部电影，有不同的人给他们打分，每部电影有不同的特征（如2个）【向量形式】；如何基于这些信息来构建一个推荐系统算法呢？——假设用线性回归模型：</p><p>$\theta^{(j)}$是用户j的参数向量（打分情况）</p><p>$x^{(i)}$是电影i的特征向量（浪漫程度、动作程度等）</p><p>对于用户j和电影i，我们预测评分为：</p><script type="math/tex; mode=display">(\theta^{(i)})^Tx^{(i)}</script><p>之后参照线性回归和梯度下降求解。</p><h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><p>如果我们没有用户的参数（评分），也没有电影的特征（浪漫程度、动作程度），可以通过协同过滤算法同时学习两者：</p><p>优化目标（代价函数）更改为同时针对x和$\theta$进行，具体步骤如下：</p><p>1.初始的x和$\theta$为一些随机的小值；</p><p>2.使用梯度下降算法最小化代价函数；</p><p>3.训练完算法后，预测用户j给电影i的评分。</p><h3 id="向量化：低秩矩阵分解（？）"><a href="#向量化：低秩矩阵分解（？）" class="headerlink" title="向量化：低秩矩阵分解（？）"></a>向量化：低秩矩阵分解（？）</h3><h3 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h3><p>当某用户没有任何基础参数时，怎么办呢？→对所有数据进行均值归一化处理（减去均值），利用处理后的新矩阵来训练算法，用新训练出的算法预测评分后，将平均值加回去！即预测：</p><script type="math/tex; mode=display">(\theta^{(i)})^T+\mu_i</script><h2 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h2><p>当有大量数据（如1000w条）作为训练集时，我们如何应对？</p><p>可以作出交叉验证集的代价函数和训练集代价函数随着训练样本数增加的变化曲线，或许只需要1000个训练集就能取得很好的结果。</p><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>将代价函数定义为一个单一训练实例的代价：</p><script type="math/tex; mode=display">cost(\theta,(x^{(i)},y^{(i)})=\cfrac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2</script><p><strong>随机</strong>的意思即，对训练集随机“洗牌”，然后：</p><script type="math/tex; mode=display">Repeat(usually 1-10){   for i=1:m{       \theta:=\theta_j-\alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}     (for j=0:n)   }}</script><p>其好处在于每一次计算后都更新$\theta$，而不需将大量训练集求和。缺点是，并不是每一步都朝正确的方向走去，甚至有可能无法走到最小值的那一点，而是在最小值点附近徘徊。</p><h3 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h3><p>介于批量梯度下降和随机下降算法之间，每计算常数b次训练实例，便更新一次参数$\theta$，一般令b=2-100，从而可以用项量化的方式来循环b个训练实例。</p><h3 id="随机梯度下降收敛"><a href="#随机梯度下降收敛" class="headerlink" title="随机梯度下降收敛"></a>随机梯度下降收敛</h3><p>在随机梯度下降中，每一次更新$\theta$之前都计算一次代价，然后每x次迭代后，求出这x次对训练实例计算代价的平均值，然后绘制这些平均值与x迭代的次数之间的函数图表。</p><p>1.函数图像颠簸不平，但<strong>不明显减少</strong>：考虑增大$\alpha$</p><p>2.函数图像<strong>不断上升</strong>：考虑减小$\alpha$，如令学习率随迭代次数的增加而减小，比如：</p><script type="math/tex; mode=display">\alpha=\cfrac{const1}{iterationNumber+const2}</script><h3 id="在线学习"><a href="#在线学习" class="headerlink" title="在线学习"></a>在线学习</h3><p>有连续的数据流，而非静态的数据集。</p><p>因而也是对一个单一实例进行训练，而非训练集！且一旦一个数据学习完成了，可以丢弃之。</p><h3 id="映射化简和数据并行"><a href="#映射化简和数据并行" class="headerlink" title="映射化简和数据并行"></a>映射化简和数据并行</h3><p>把数据集分配给多台计算机，每个计算机求一个子集，再将结果汇总。（或者利用多核CPU的多个核心进行并行运算）<br>假设400个训练实例被分配到4台计算机进行处理，最后</p><script type="math/tex; mode=display">\theta_j=\theta_j-\alpha\cfrac{1}{400}(temp^{(1)})(temp^{(2)})(temp^{(3)})(temp^{(4)})</script><h2 id="应用实例：图片文字识别"><a href="#应用实例：图片文字识别" class="headerlink" title="应用实例：图片文字识别"></a>应用实例：图片文字识别</h2><p>从图片中识取文字并辨认出来：</p><p>1.文字侦测——将图片上的文字与其他环境对象分离</p><p>2.字符切分——将文字分割成一个个单一的字符</p><p>3.字符分类——确定每一个字符是什么</p><h3 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h3><p>按比例框定窗口，按比例放大缩小扫描整张图片。交给模型进行评判。完成识别后，还可以将识别区域进行扩展。</p><h3 id="获得大量数据和人工数据"><a href="#获得大量数据和人工数据" class="headerlink" title="获得大量数据和人工数据"></a>获得大量数据和人工数据</h3><p>人工数据——如识别文字，可以用字体库+背景自己创造数据</p><p>已有数据修改——如文字图片的扭曲、旋转和模糊处理</p><p>手动标记或众包</p><h3 id="上限分析"><a href="#上限分析" class="headerlink" title="上限分析"></a>上限分析</h3><p>在改进算法的过程中，假设某一个步骤（人工地使之）完全正确，对比不人工处理情况下的预测准确率，对不同的步骤都进行处理，选择改进空间最大的着力改进。</p><p>—</p><p>以上，机器学习的笔记暂时告一段落。这些都是暑假后半段自学的内容，再输出一遍同时自己也复习一下。如果有补充可能会直接文内更新/在新文章里加对应tag。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第6篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://li-zhe.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="大规模机器学习" scheme="http://li-zhe.com/tags/%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(5)无监督学习(Unsupervised Learning)及异常检测(Outlier Detection)</title>
    <link href="http://li-zhe.com/posts/20191220/"/>
    <id>http://li-zhe.com/posts/20191220/</id>
    <published>2019-12-20T02:17:34.000Z</published>
    <updated>2020-07-25T13:38:24.662Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第5篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><h3 id="K-means聚类"><a href="#K-means聚类" class="headerlink" title="K-means聚类"></a>K-means聚类</h3><p>首先选择K个随机的点，为<strong>聚类中心</strong>，然后对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点归为一类。再计算每个类的平均值，把该类的中心点移动到平均值的位置，重复对数据点进行聚类。——直到中心点不再变化。</p><h4 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h4><p>随机选择K&lt;m（训练集实例数量）个训练实例，然后令K个聚类中心分别与这K个训练实例相等。</p><p>为了解决局部最小值问题，通常进行多次K-means运算，每一次都重新进行随机初始化，最后再比较结果。</p><h4 id="选择聚类数（肘部法则）"><a href="#选择聚类数（肘部法则）" class="headerlink" title="选择聚类数（肘部法则）"></a>选择聚类数（肘部法则）</h4><p>更改K的值，直到代价函数下降较慢。（类似PCA碎石图）</p><h3 id="数据降维"><a href="#数据降维" class="headerlink" title="数据降维"></a>数据降维</h3><p>从二维降到一维 → 如将二元坐标点投影到一条线段上。</p><p>从三维到二维 → 如将三元坐标点投影到一个二维平面上。</p><p>数据可视化？ → 新产生特征的意义需要自己去发现。</p><h4 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h4><p>第一步，均值归一化，计算出所有特征的均值，然后令$x_j=x_j-\mu_j$，如果特征在不同数量级上，还需要除以标准差$\sigma^2$。</p><p>第二步，计算协方差矩阵：</p><script type="math/tex; mode=display">\Sigma=\cfrac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T</script><p>第三步，计算协方差矩阵的特征向量：</p><script type="math/tex; mode=display">[U,S,V] = svd(\Sigma)</script><p>U即是一个具有与数据之间最小投射误差的方向向量构成的矩阵，若想将数据从n维降到k维，只需从U中选取前k个向量，得到一个新的n×k的矩阵用$U_reduce$表示，然后计算得新的特征向量：</p><script type="math/tex; mode=display">z^{(i)}=U_{reduce}^T*x^{(i)}</script><p>x是n×1维的，所以结果为k×1维。</p><p>而S是一个n×n的矩阵，只有对角线上有值，可以用来计算平均均方误差与训练集方差的比例（小于1%为例）：</p><script type="math/tex; mode=display">\cfrac{\sum_{i=1}^{k}S_{ii}}{\sum_{i=1}^{n}S_{ii}}>=0.99</script><p>压缩数据后，近似地获得原有特征：</p><script type="math/tex; mode=display">x^{(i)}_{approx}=U_{reduce}^T*z^{(i)}</script><h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p>给定数据集，已知数据集正常，那么如何判断一个新的数据是不是异常值，以及这个新数据不属于这一组数据的几率如何？</p><h3 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h3><p>高斯分布即正态分布，符合之的变量$x~N(\mu,\sigma^2)$,其概率密度函数为：</p><script type="math/tex; mode=display">p(x,\mu,\sigma^2)=\cfrac{1}{\sqrt{2\pi}\sigma}exp{-\cfrac{(x-\mu)^2}{2\sigma^2}}</script><p>其中的$\mu$和$\sigma^2$分别为均值和方差，$\mu$影响中点位置，$\sigma^2$影响峰的宽度高度。</p><h3 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h3><p>给定数据集$x^{(1)}-x^{(m)}-$，则针对每一个特征计算mu和sigma的估计值：</p><script type="math/tex; mode=display">\mu_j=\cfrac{1}{m}\sum_{i=1}^{m}x_j^{(i)}</script><script type="math/tex; mode=display">\sigma_j^2=\cfrac{1}{m}\sum_{i=1}^{m}(x_j^(i)-\mu_j)^2</script><p>获得两个估计值后，和给定的新训练实例$x_j$一起代入高斯分布计算p(x)：</p><script type="math/tex; mode=display">p(x)=\prod^{n}_{j=1}p(x_j;\mu_j;\sigma_j^2)</script><p>当$p(x)&lt;\epsilon$时即为<strong>异常</strong>。</p><p>注意：异常检测与监督学习有些相似，但最大的不同在于异常检测的正向类（异常数据y=1）非常少，负向类非常多（正常数据y=0）！而监督学习同时拥有大量的正和负类。</p><h3 id="开发异常检测系统"><a href="#开发异常检测系统" class="headerlink" title="开发异常检测系统"></a>开发异常检测系统</h3><p>作为一个非监督学习算法，在用其开发一个异常检测系统时，需要注意数据的选取：</p><p>如有10000个正常数据，其中有20个异常，那么建议如下<strong>分配</strong>：</p><p>6000个正常数据作为训练集；</p><p>2000个正常数据和10个异常数据作为交叉检验集；</p><p>2000个正常数据和10个异常数据作为测试集。</p><p>而<strong>评价方法</strong>如下：</p><p>1.根据训练集数据，估计特征的均值和方差，并构建p(x)；</p><p>2.对交叉检验集，用不同的$\epsilon$值作阈值，预测数据是否异常；并根据F1选择$\epsilon$；</p><p>3.选出$\epsilon$后，针对测试集进行预测，计算F1值。</p><h3 id="选择特征"><a href="#选择特征" class="headerlink" title="选择特征"></a>选择特征</h3><p>如果数据分布不是高斯分布，可以通过对数变换或者指数变换使之成为高斯分布。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第5篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://li-zhe.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="异常检测" scheme="http://li-zhe.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(4)支持向量机(Support Vector Machine)</title>
    <link href="http://li-zhe.com/posts/20191219/"/>
    <id>http://li-zhe.com/posts/20191219/</id>
    <published>2019-12-19T02:44:34.000Z</published>
    <updated>2020-07-25T13:38:06.572Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第4篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h2><h3 id="从逻辑回归开始"><a href="#从逻辑回归开始" class="headerlink" title="从逻辑回归开始"></a>从逻辑回归开始</h3><p>逻辑回归的假设：</p><script type="math/tex; mode=display">h_\theta(x)=\cfrac{1}{1+e^{-\theta^Tx}}</script><p>若y=1，我们希望$h_\theta(x)\approx1$，$\theta^Tx\gg0$</p><p>若y=0，则$h_\theta(x)\approx0$，$\theta^Tx\ll0$（注意考虑S型的函数）</p><p>考虑到代价函数：</p><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><p>y=0或y=1时代价函数会消掉一部分，导致$\theta^Tx$极大或极小能确保代价函数趋于0。</p><p>据此构建支持向量机：</p><script type="math/tex; mode=display">\underbrace{min}_{\theta}C\sum^{m}_{i=1}[y^{(i)}cost_1(\theta^Tx^{(i)}+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\cfrac{1}{2}\sum_{j=1}^{n}\theta_j^2</script><p>此处的C，相当于之前的$\cfrac{1}{\lambda}$，因此：</p><p>C较大时，相当于$\lambda$较小，可能导致过拟合，高方差；</p><p>C较小时，相当于$\lambda$较大，可能导致低拟合，高偏差。</p><p>向量u的范数=$\sqrt{u_1^2+u_2^2}$，代表u的欧几里得长度。</p><p>通过向量内积：</p><script type="math/tex; mode=display">\vec u^T\vec v=\vec v^T\vec u=||u||p</script><p>注意p和$||u||$都是实数，p作为投影是有方向的（正负）。</p><p>从而实现：</p><script type="math/tex; mode=display">\theta^Tx^{(i)}=p^{(i)}·||\theta||</script><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>如果说模型是：</p><script type="math/tex; mode=display">\theta_0+\theta_1x_1+\theta_2x_2+...\theta_4x1^2+...</script><p>那么我们可以用一系列新的特征f来替换模型中的每一项，如：</p><script type="math/tex; mode=display">f_1=x_1,f_2=x_2,f_3=x_1^2...</script><p>然而除了对原有特征进行组合以外，我们还可以用<strong>核函数</strong>来构建新的特征。</p><p>给定一个训练实例x，我们可以利用x的各个特征与我们预先选定的landmarks，$l^{1},l^{(2)},l^{(3)}$的近似程度来选新的特征$f_1,f_2,f_3$，例如：</p><script type="math/tex; mode=display">f_1=similarity(x,l^{(1)})=e(-\cfrac{||x-l^{(1)}||^2}{2\sigma^2})</script><p>其中：</p><script type="math/tex; mode=display">||x-l^{(1)}||^2=\sum_{j=1}^{n}(x_j-l_j^{(1)})^2</script><p>即x中的所有特征与$l^{(1)}$的距离之和，上面的similarity就是一个高斯核函数。</p><p>如果x与l之间距离近似于0，则新特征f→1；反之较远则f→0。</p><p>（SVM也可以不使用核函数，不使用又称为线性核函数，一般训练集特征非常多而实例非常少的时候可以采用）</p><p>下面是支持向量机的参数C（之前有）和$\delta$的影响：</p><p>1.$\delta$较大时，可能导致低方差，高偏差；</p><p>2.$\delta$较小时，可能导致低偏差，高方差。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第4篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="支持向量机" scheme="http://li-zhe.com/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(3)神经网络(Neural Network)及模型改进</title>
    <link href="http://li-zhe.com/posts/20191218/"/>
    <id>http://li-zhe.com/posts/20191218/</id>
    <published>2019-12-18T01:44:34.000Z</published>
    <updated>2020-07-25T13:37:37.930Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第3篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>训练样本个数：</p><script type="math/tex; mode=display">m</script><p>每个样本包含一组输入和一组输出：</p><script type="math/tex; mode=display">x，y</script><p>表示神经网络层数Layer：</p><script type="math/tex; mode=display">L</script><p>表示每层的神经元个数：</p><script type="math/tex; mode=display">S_I</script><p>表示输出层神经元个数：</p><script type="math/tex; mode=display">S_l</script><p>表示最后一层中处理单元的个数：</p><script type="math/tex; mode=display">S_L</script><h3 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h3><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}[\sum_{i=1}^m\sum_{k=1}^ky_k^{(i)}log(h_\theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\theta(x^{(i)}))_k)]+\cfrac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{ji}^{(l)})^2</script><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>为了计算代价函数的偏导数$\cfrac{\partial}{\partial\theta_{ij}^{(l)}}$,先计算最后一层的误差，再一层一层反向求出各层误差，直到倒数第二层。</p><h3 id="（神经网络部分细节）亟待补充"><a href="#（神经网络部分细节）亟待补充" class="headerlink" title="* （神经网络部分细节）亟待补充"></a>* （神经网络部分细节）亟待补充</h3><h3 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h3><p>1.参数的随机初始化</p><p>2.利用正向传播方法计算所有的$h_\theta(x)$</p><p>3.编写计算代价函数J的代码</p><p>4.利用反向传播方法计算所有偏导数</p><p>5.利用数值检验方法检验偏导数</p><p>6.利用优化算法来最小化代价函数</p><h2 id="模型应用建议（不只针对神经网络）"><a href="#模型应用建议（不只针对神经网络）" class="headerlink" title="模型应用建议（不只针对神经网络）"></a>模型应用建议（不只针对神经网络）</h2><p>当运用训练好了的模型预测未知数据时发现有较大误差，下一步？</p><p>1.减少特征的数量；</p><p>2.获取更多特征；</p><p>3.增加多项式特征（如$x^2，x_1·x_2$等）</p><p>4.减少or增加正则化程度$\lambda$</p><h3 id="对假设的评估"><a href="#对假设的评估" class="headerlink" title="对假设的评估"></a>对假设的评估</h3><p>通常，数据=70%训练集+30%测试集；获得模型后，可以：</p><p>1.线性回归：直接用测试集数据计算代价函数J；</p><p>2.逻辑回归：计算代价函数外，还可计算错误率；</p><h3 id="模型选择和交叉验证集"><a href="#模型选择和交叉验证集" class="headerlink" title="模型选择和交叉验证集"></a>模型选择和交叉验证集</h3><p>即，<strong>数据=60%训练集+20%交叉验证集+20%测试集</strong>；之后：</p><p>1.使用<strong>训练集</strong>训练处n个模型；</p><p>2.用n个模型分别对<strong>交叉验证集</strong>计算得出交叉验证误差（代价函数值）；</p><p>3.选取代价函数值最小的模型；</p><p>4.用3中选出的模型对<strong>测试集</strong>计算得出推广误差（代价函数值）。</p><h3 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h3><p>偏差较大→欠拟合；方差较大→过拟合。</p><p>可以通过将训练集和交叉验证集的代价函数与<strong>多项式的次数</strong>绘制在同一张图表上来帮助分析，训练集代价函数肯定是随次数上升减小的，而交叉验证集是U形：</p><p>二者相近：偏差/欠拟合</p><p>交叉远大于训练：方差/过拟合</p><p>而<strong>正则化系数$\lambda$也</strong>有所贡献，训练集误差随着$\lambda$而增加，交叉验证集则随$\lambda$增加先减小后增加；因为$\lambda$增加使得$\theta$的贡献降低了：</p><p><strong>学习曲线</strong></p><p>将训练集误差和交叉验证集误差作为训练集实例数量（m）函数而绘制的图→说明高方差/过拟合情况下增加更多数据可能提升算法效果。</p><h3 id="建议Solution"><a href="#建议Solution" class="headerlink" title="建议Solution"></a>建议Solution</h3><p>1.获取更多的数据（训练实例）→高方差√ </p><p>2.减少特征数量→高方差√</p><p>3.增加特征数量→高偏差√</p><p>4.增加多项式特征→高偏差√</p><p>5.减少正则化程度$\lambda$→高偏差√</p><p>6.增加正则化程度$\lambda$→高方差√</p><h3 id="类偏斜"><a href="#类偏斜" class="headerlink" title="类偏斜"></a>类偏斜</h3><p>一个极端的例子，假设在用算法预测癌症是良性还是恶性，那么如果说训练集的样本中只有0.5%是恶性，其余全是良性；那么我们平白无故算法A预测所有肿瘤都是良性，误差为0.5%；而写出来的神经网络算法B训练后误差可能有1%，这个时候显然不能根据误差判断A比B算法好。</p><p><strong>查准率precision=TP/(TP+FP)</strong>，即所有预测有恶性肿瘤的病人中，实际有恶性肿瘤的百分比，越↑越好。</p><p><strong>查全率recall=TP/(TP+FN)</strong>，即所有实际有恶性肿瘤的病人中，被预测到的百分比，越↑越好。</p><p>如何权衡这两个指标呢？根据需求！</p><p>可以在不同阈值的情况下，将二者关系绘制成图表（x/y轴是二者）；一个帮助选择阈值的方法：<br><strong>F1值法</strong>：</p><script type="math/tex; mode=display">F1=\cfrac{2PR}{P+R}</script><p>选择使F1最高的阈值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第3篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://li-zhe.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模型改进" scheme="http://li-zhe.com/tags/%E6%A8%A1%E5%9E%8B%E6%94%B9%E8%BF%9B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(2)逻辑回归(Logistic Regression)及正则化(Normalization)</title>
    <link href="http://li-zhe.com/posts/20191217/"/>
    <id>http://li-zhe.com/posts/20191217/</id>
    <published>2019-12-17T06:44:34.000Z</published>
    <updated>2020-07-25T13:36:56.096Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第2篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>其余篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><h2 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归(Logistic Regression)"></a>逻辑回归(Logistic Regression)</h2><p>引入模型，使得模型的输出变量范围始终在0和1之间，以便完成分类：</p><script type="math/tex; mode=display">h_\theta(x)=g(\theta^TX)</script><p>X代表特征向量,g代表逻辑函数，此处为Sigmoid函数：</p><script type="math/tex; mode=display">g(z)=\cfrac{1}{1+e^{-z}}</script><p>故：</p><script type="math/tex; mode=display">h_\theta(x)=\cfrac{1}{1+e^{-\theta^TX}}</script><p>此时h的作用是对于给定的输入变量，输出变量=1的可能性，即：</p><script type="math/tex; mode=display">h_\theta(x)=P(y=1|x;\theta)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br></pre></td></tr></table></figure><p>我们预测的规则是h&gt;0.5则y=1,反之y=0；又因为g(z)的0.5分界在于z=0，所以$\theta^Tx?0$是判断的边界。故决策边界（曲线）方程即是：</p><script type="math/tex; mode=display">\theta^Tx=0</script><p>线性回归的代价函数：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>逻辑回归的代价函数：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)})-y^{(i)})</script><p>y=1时：</p><script type="math/tex; mode=display">Cost(h_\theta(x),y)=-log(h_\theta(x))</script><p>y=0时：</p><script type="math/tex; mode=display">Cost(h_\theta(x),y)=-log(1-h_\theta(x))</script><p>简化之：</p><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(theta,X,y)</span>:</span></span><br><span class="line">    theta=np.matrix(theta)</span><br><span class="line">    X=np.matrix(X)</span><br><span class="line">    y=np.matrix(y)</span><br><span class="line">    first=np.multiply(y,np.log(sigmoid(X*theta.T)))</span><br><span class="line">    second=np.multiply((<span class="number">1</span>-y),mnp.log(<span class="number">1</span>-sigmoid(X*theta.T)))</span><br><span class="line">    <span class="keyword">return</span> np.sum(first+second)/-(len(X))</span><br></pre></td></tr></table></figure><p>仍可以用梯度下降算法求使代价函数最小的参数：</p><script type="math/tex; mode=display">\theta_j=\theta_j-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x_j^{(i)})</script><p>易知代价函数会带来一个凸优化问题，即$J(\theta)$会是一个凸函数，并且没有局部最优值。</p><p>除了梯度下降之外，常用来令代价函数最小的算法：</p><ul><li>共而梯度Conjugate Gradient</li><li>局部优化法Broyden fletcher goldfarb shann,BFGS</li><li>有限内存局部优化法LBFGS</li></ul><h2 id="正则化-Normalization"><a href="#正则化-Normalization" class="headerlink" title="正则化(Normalization)"></a>正则化(Normalization)</h2><p>面对过拟合问题，一是想办法减少特征量，二便是正则化（保留所有特征但是减小参数的大小）。</p><p>过拟合来自于高次项（虚线过于扭曲），因而可以通过让高次项的系数接近于0来完成拟合。为此要在代价函数中为$\theta$添加惩罚。</p><p>此时的代价函数为：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]</script><p>$\lambda$即为正则化参数，过大会导致模型欠拟合，过小则导致所有$\theta$都趋于0。</p><h3 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h3><p>代价函数：</p><script type="math/tex; mode=display">J(\theta)=\cfrac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]</script><p>梯度下降（未对$\theta_0$正则化）：</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha[\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\cfrac{\lambda}{m}\theta_j]</script><p>正则回归(矩阵尺寸为(n+1)*(n+1))：</p><script type="math/tex; mode=display">\theta=(X^TX+\lambda[0 1 1 ... 1])^{-1}X^Ty</script><h3 id="正则化逻辑回归"><a href="#正则化逻辑回归" class="headerlink" title="正则化逻辑回归"></a>正则化逻辑回归</h3><p>代价函数：</p><script type="math/tex; mode=display">J(\theta)=-\cfrac{1}{m}\sum_{i=1}^m[y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\cfrac{\lambda}{2m}\sum_{j=1}^n\theta_j^2</script><p>梯度下降（未对$\theta_0$正则化）：</p><script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha[\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\cfrac{\lambda}{m}\theta_j]</script><p>看上去和线性回归一样，但是$h_\theta(x)=g(\theta^TX)$。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第2篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://li-zhe.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="正则化" scheme="http://li-zhe.com/tags/%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(1)单变量线性回归及多维特征情形</title>
    <link href="http://li-zhe.com/posts/20191216/"/>
    <id>http://li-zhe.com/posts/20191216/</id>
    <published>2019-12-16T01:44:34.000Z</published>
    <updated>2020-07-25T13:36:47.203Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为吴恩达《机器学习》课程学习笔记的第1篇；<br>视频/作业/PPT等资源可在<a href="https://zh.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a>和<a href="https://study.163.com/course/courseLearn.htm?courseId=1004570029#/learn/video?lessonId=1049052745&amp;courseId=1004570029" target="_blank" rel="noopener">网易云课堂</a>等处找到；<br>后续篇章可见左侧Cate里的“Andrew Ng《Machine Learning》笔记”。</p></blockquote><p><strong>开篇，关于机器学习的定义：</strong></p><blockquote><p>TomMitchell:<br>一个程序从经验E中学习，解决任务T，达到性能度量值P；当且仅当有了经验E后，经P评判，程序处理T时的性能有所提高。</p></blockquote><h2 id="从单变量的线性回归开始"><a href="#从单变量的线性回归开始" class="headerlink" title="从单变量的线性回归开始"></a>从单变量的线性回归开始</h2><p>特征的数量：<script type="math/tex">1</script><br>训练集中实例的数量：<script type="math/tex">m</script><br>特征/输入变量：<script type="math/tex">x</script><br>目标/输出变量：<script type="math/tex">y</script><br>训练集中的实例：<script type="math/tex">(x,y)</script><br>训练集中第i个实例：<script type="math/tex">(x^{(i)},y^{(i)})</script><br>函数/假设：<script type="math/tex">h</script><br>单变量线性回归问题：<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x</script><br>代价函数（建模误差）：<script type="math/tex">J(\theta_0,\theta_1)=\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><br>我们的目标是建立模型h，使得代价函数J最小，即：<script type="math/tex">\underbrace{minimize}_{\theta_0,\theta_1}J(\theta_0,\theta_1)</script></p><h3 id="批量梯度下降-同时更新2个theta-："><a href="#批量梯度下降-同时更新2个theta-：" class="headerlink" title="批量梯度下降(同时更新2个theta)："></a>批量梯度下降(同时更新2个theta)：</h3><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)</script><p>（更新theta的原理在于后面的导数项，通过斜率正负确保随着theta的增大或减小，J(theta）一定随之减小，同时因为J(theta)的斜率变化，theta的变化也随着J的减小而减小，如在接近局部最低点时导数接近0，此时梯度下降法会自动采取更小的幅度）。</p><p>所以梯度下降的实现关键在于求出代价函数J的导数：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\cfrac{\partial}{\partial\theta_j}\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>j=0：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\cfrac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})</script><p>j=1：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\cfrac{1}{2m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x^{(i)})</script><h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><p>通过求解方程来找出使J最小的参数(不可逆矩阵不可用)：</p><script type="math/tex; mode=display">\cfrac{\partial}{\partial\theta_j}J(\theta_j)=0</script><p>假设训练集的特征矩阵为X，训练集的结果为向量y，则：</p><script type="math/tex; mode=display">\theta=(X^TX)^{(-1)}X^Ty</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(X,y)</span>:</span></span><br><span class="line">    theta=np.linalg.inv(X.T@X)@X.T@y</span><br><span class="line">    <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure><h2 id="多维特征情形"><a href="#多维特征情形" class="headerlink" title="多维特征情形"></a>多维特征情形</h2><h3 id="通用情形"><a href="#通用情形" class="headerlink" title="通用情形"></a>通用情形</h3><p>特征的数量：<script type="math/tex">n</script><br>训练集中的实例成为行向量：<script type="math/tex">x^{(i)}</script><br>第i个训练实例的第j个特征：<script type="math/tex">x_j^{(i)}</script><br>X是类似m×n的矩阵，m是实例个数，n是特征个数</p><p>多变量的假设：<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script><br>为了简化公式，引入$x_0=1$：</p><script type="math/tex; mode=display">h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script><p>即：<script type="math/tex">h_\theta(x)=\theta^T(X)</script><br>我们的目标和单变量情形下一样，只不过theta更多：</p><script type="math/tex; mode=display">\theta_0=\theta_0-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x_0^{(i)})</script><script type="math/tex; mode=display">\theta_1=\theta_1-\alpha\cfrac{1}{m}\sum_{i=1}^m((h_\theta(x^{(i)})-y^{(i)})·x_1^{(i)})</script><script type="math/tex; mode=display">...</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeCost</span><span class="params">(X,y,theta)</span>:</span></span><br><span class="line">    inner=np.power(((X*theta.T)-y),<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.sum(inner)/(<span class="number">2</span>*len(X))</span><br><span class="line"><span class="comment">#len(X)代表m而不是n，注意！</span></span><br></pre></td></tr></table></figure><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h4 id="特征放缩"><a href="#特征放缩" class="headerlink" title="特征放缩"></a>特征放缩</h4><p>当两个特征值范围差的很远，如0-5和0-2000时，代价函数的等高线图能看出图像很扁，梯度下降也需要非常多次迭代才能收敛，方法是尝试将所有特征的尺度都缩放到-1到1之间：</p><script type="math/tex; mode=display">x_n=\cfrac{x_n-\mu_n}{s_n}</script><p>(平均值$\mu_n$，标准差$s_n$)</p><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><p>梯度下降算法的迭代受到学习率alpha的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如过大，则每次迭代可能不会减小代价函数，越过局部最小值而导致无法收敛。<br>建议的学习率：</p><script type="math/tex; mode=display">\alpha=0.01,0.03,0.1,0.3,1,3,10</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;本文为吴恩达《机器学习》课程学习笔记的第1篇；&lt;br&gt;视频/作业/PPT等资源可在&lt;a href=&quot;https://zh.coursera.org/learn/machine-learning&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="http://li-zhe.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://li-zhe.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="梯度下降" scheme="http://li-zhe.com/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://li-zhe.com/posts/1/"/>
    <id>http://li-zhe.com/posts/1/</id>
    <published>2019-12-13T03:00:20.236Z</published>
    <updated>2020-07-25T13:28:49.926Z</updated>
    
    <content type="html"><![CDATA[<p>欢迎你的到来，也欢迎我自己的回归。</p><hr><p>欢迎来到我的二代站点，一代见左侧<strong>Link To</strong>处“Previous Site”。</p><p>不出意外的话它将在2020年1月11日关闭，原因是<del>负担不起10块一个月的服务器费用了(并不)</del>比较复杂的。</p><p>届时可能会把域名迁移过来(<del>虽然喜新厌旧的我已经打算买新的了</del>)，反正看的人也不多，还是用git省心一点嗷。</p><p>先这样吧，等我慢慢把评论功能和题图啥的整出来再开始写文章好了。</p><p>偷偷测试一下emoji：💉💦🐂🍺👌😂</p><p>( •̀ ω •́ )y</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;欢迎你的到来，也欢迎我自己的回归。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;欢迎来到我的二代站点，一代见左侧&lt;strong&gt;Link To&lt;/strong&gt;处“Previous Site”。&lt;/p&gt;
&lt;p&gt;不出意外的话它将在2020年1月11日关闭，原因是&lt;del&gt;负担不起10块一个月的服
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>可能是建立网站的初衷</title>
    <link href="http://li-zhe.com/posts/20191114/"/>
    <id>http://li-zhe.com/posts/20191114/</id>
    <published>2019-11-14T07:24:59.000Z</published>
    <updated>2020-08-05T12:54:37.601Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>我建立的第一个网站www.justmonika.xyz在2020年1月11日正式关闭了。<br>以下文字由其历史内容整合而成，主要是面临放弃旧站时，对是否要继续写新网站的思考，原文标题为《会半途而废吗？》。<br>“注”的内容是后来添加的<del>吐槽自己还是拿手啊</del>。</p></blockquote><h1 id="可能是建立网站的初衷"><a href="#可能是建立网站的初衷" class="headerlink" title="可能是建立网站的初衷"></a>可能是建立网站的初衷</h1><p>时隔上一次更新已经过去了快一个月，起初建站没有明确的目标，只是想熟悉一下整个流程，多鼓捣鼓捣Linux。建成之始还是很有成就感，待到公安备案完成，一切尘埃落定，突然发觉不需要再做什么，顿感迷茫。</p><p>思考了几天后意图把这个网站作为自己输出的平台，分享一些日常和学习经验。</p><p>然而域名已经给百度检索提交了几个礼拜，还是很难检索到内容。我又没脸往朋友圈推广自己的网站，也不是什么勤勉的高质量日更博主能吸引来路人，恐怕阅读量会持续维持在0-1之间。</p><p>为了测试，我写过一篇简短的博文，分别发在了CSDN/博客园等等的不同的平台，得到的反馈明显比自己瞎建的网站要好。可惜商业博客虽方便，能自定义的东西却太少，广告又太多，并不适合我。</p><p>近段时间顺着网上各种资源找了不少做生信的大佬们的网站，虽然不至于像以前那样只能在心里默念牛逼了，但还是不得不感慨别人的细节和精致<sup>注1</sup>。这或许也是我继续建站的动力之一吧？</p><p>（注1：后来也发现不少做得丑丑水水的，模板使用之敷衍令人痛心疾首，不提）</p><p>这么一反思，我突然发现我的域名<sup>注2</sup>如此拗口、页面毫无新意、功能不见特色，简直一无是处。再仔细一想，平常我可以在微博倒垃圾，在朋友圈发心情，在知乎答问题；学习笔记自己记完了往大象一传就完事，我似乎没什么非写在博客里不可的东西。</p><p>（注2：当时的域名justmonika出自游戏《Doki Doki Literature Club!》，中文译名《心动文学社》。作为我玩的第一部Galgame也是迄今玩过最棒的Galgame，我在此向所有没有心脏疾病的玩家推荐它！毕竟也有人说这是恋爱模拟做得最好的恐怖游戏<del>（笑）</del>尤其是2周目、3周目的剧情令人心动不已，完美点题Doki Doki！不过好玩的前提的你<strong>不被剧透</strong>。）</p><p><strong>那么，是不是可以放弃这个网站了呢？</strong></p><p>不可以的，我最恨自己做事情半途而废了。</p><p>但这不是主要原因。</p><p>我想，主要原因是我期待一个真实地、不加修饰地表达自己的地方。不知道什么时候起我很少发朋友圈/空间了，甚至QQ空间已经在Tim里关闭很久没有打开过了，总觉得被动接收自己不感兴趣的信息<sup>注3</sup>是一种负担。</p><p>（注3：我知道有的人只是单纯地分享自己的生活，但也有些朋友喜欢把熬夜写（莫名其妙的）材料当成鸡汤给自己灌还每天都要发，我是真的不想看）</p><p>往外输出也是一样，一旦我发送出去了，就意味着所有好友都能看到。也许有人会出于关心或者只是好奇想知道：我脱单了吗？保研去了哪里？做什么方向？但显然并不是每个人都想知道我“今天早上吃了什么？为什么要建网站呢？又有什么感想？”，所以我在写朋友圈/空间的时候总是会思考，该写吗，写什么，父母看到会怎么想，老师呢，会因为吐槽被不明真相的朋友对号入座产生误解吗，等等外校老师看到了可能不太好要拉个分组……<br>太累了。</p><p>这不真情实感，而是在给朋友圈的各位塑造自己的形象<sup>注4</sup>，和不少人往朋友圈发自拍一定要P得漂漂亮亮，拍了风景/美食要选图调色，加上地点tag大声宣布：“我在xx旅游/吃饭了！”一个道理。合情合理，不过我有些疲于应对。结果就是我很少发表主观的观点，大多是随口吐槽一下日常生活。</p><p>（注4：或许这就是朋友圈存在的意义之一）</p><p>所有带有“好友”、“关注”属性的社交app，网站于我而言都有这个毛病，我不希望看到不感兴趣的动态，更不希望不想看到我的动态的人被迫看着我的动态<sup>注5</sup>。</p><p>（注5：近来微博好友也变多了，有不想看我话痨的朋友请务必随意取关我，我绝无任何怨言，也绝不会对我们的关系产生任何影响。千万不要“出于礼貌”关注着我，却其实并不想看我发微博。）</p><p>网站的话，似乎解决了不少问题。想看的可以自己主动点进来看或者写评论，我也不知道是谁在看、谁在评论，自然也不用考虑观众们的感受。看完了就可以走，没兴趣直接右上角。于我而言希望每个人都有个人主页，这样我就可以选择性订阅想看的人，偶尔看看许久未见的人，一次性直接看完最近的所有事情，之后或许可以找些感兴趣的话题私聊叙旧一会儿，或许好奇心得到满足也就离开。</p><p>偏题了。</p><p>回答一下自己的问题：不会半途而废。</p><p>相反我希望做得更好，也许是更好的网站，也许会尝试一下做视频（B站老会员了，可惜至今没传过视频……）？总之有一块属于自己的地方很舒坦，没有阴阳怪气冷嘲热讽<sup>注6</sup>，不用担心被熟人看见留下不好的印象；不用担心被误解导致关系僵硬。<del>更不用怕万一被有好感的女生看见了心想”原来他是这样的人！”</del></p><p>（注6：也许未来会有，甚至有点期待）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;我建立的第一个网站www.justmonika.xyz在2020年1月11日正式关闭了。&lt;br&gt;以下文字由其历史内容整合而成，主要是面临放弃旧站时，对是否要继续写新网站的思考，原文标题为《会半途而废吗？》。&lt;br&gt;“注”的内容是后来添加的&lt;del&gt;
      
    
    </summary>
    
    
      <category term="个人随想" scheme="http://li-zhe.com/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="个人随想" scheme="http://li-zhe.com/tags/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>真正的Github新手(指自己)入门指南</title>
    <link href="http://li-zhe.com/posts/20191006/"/>
    <id>http://li-zhe.com/posts/20191006/</id>
    <published>2019-10-06T07:24:59.000Z</published>
    <updated>2020-08-05T12:52:35.430Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>这篇文章是我入门Github时期记录的笔记，当时主要是参考<a href="https://book.douban.com/subject/26462816/" target="_blank" rel="noopener">《Github入门与实践》[日]大塚弘记</a>一书，文中有部分内容也来源于此。<br>个人感觉比较适合新手入门的，感兴趣的朋友可以自行阅读。</p></blockquote><p>前面的简介和账号创建直接略过，从需要输入代码的部分开始：</p><h2 id="3-使用前的准备"><a href="#3-使用前的准备" class="headerlink" title="3 使用前的准备"></a>3 使用前的准备</h2><h4 id="3-1-设置SSH-Key"><a href="#3-1-设置SSH-Key" class="headerlink" title="3.1 设置SSH Key"></a>3.1 设置SSH Key</h4><p>第一个问题出现了，什么是SSH Key呢？简单搜索了一下还挺复杂，但我的理解是GitHub通过SSH Key与本地的仓库相连接，也就是说我们在本地创建SSH Key然后用它连接到GitHub就行了。打开下载好的GitBash开始输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">"email@xxx.com"</span></span><br></pre></td></tr></table></figure><p>运行完成后设置一下密码。设置完成后系统会告诉你id_rsa和id_rsa.pub的位置以及key的指纹和图像（fingerprint,randomart image）分别是什么。<br>其中id_rsa是私有密钥，id_rsa.pub是<strong>公开密钥</strong>，GitHub即通过<strong>公开密钥</strong>认证来连接已有仓库。</p><h4 id="3-2-添加公开密钥"><a href="#3-2-添加公开密钥" class="headerlink" title="3.2 添加公开密钥"></a>3.2 添加公开密钥</h4><p>先通过：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure><p>查看自己的Key，然后在GitHub网页上找到Add SSH Key的入口，Title处输入密钥名称，Key部分粘贴Key内容即可。（注意Key不包括邮件但包括ssh-rsa）完成后会收到提示邮件。</p><p>文中提到完成设置之后就可以用私人密钥与GitHub进行认证和通信了，即运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>不过公开密钥和私有密钥的区别是什么呢？我搜了半天也没有找到明确的解释，我猜是公开密钥用来连接本地仓库和远程仓库，私人密钥来确认文件的传输吧。</p><h2 id="4-基本操作"><a href="#4-基本操作" class="headerlink" title="4 基本操作"></a>4 基本操作</h2><h4 id="4-1-本地仓库初始化"><a href="#4-1-本地仓库初始化" class="headerlink" title="4.1 本地仓库初始化"></a>4.1 本地仓库初始化</h4><p>这部分是关于本地的Git仓库的基本操作。<br>首先打开GitBash，建立一个目录并初始化仓库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir git-tutorial</span><br><span class="line">$ <span class="built_in">cd</span> git-tutorial</span><br><span class="line">$ git init</span><br></pre></td></tr></table></figure><p>成功后会生成.git目录，其内存放的便是管理当前目录内容所需的仓库数据。称之为“工作树”，文件编辑在工作树中进行，仓库内是记录和快照。</p><p>创建+初始化仓库完成后，可以查看其状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git status</span><br></pre></td></tr></table></figure><p>结果会显示①当前位置（分支）②有无可提交内容；显然新建的仓库肯定啥都没有。</p><p>于是建立一个README.md文件进行实验：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立文件</span></span><br><span class="line">$ touch README.md</span><br><span class="line">$ git status</span><br><span class="line"><span class="comment">#把它添加到暂存区</span></span><br><span class="line">$ git add README.md</span><br><span class="line">$ git status</span><br><span class="line"><span class="comment">#保存历史记录</span></span><br><span class="line">$ git commit -m <span class="string">"first commit"</span></span><br><span class="line">$ git status</span><br></pre></td></tr></table></figure><p>这里提到了git commit命令——保存仓库的历史记录，我记得之间用push把本地文件传到GitHub之前也有git commit命令，之前一直没搞清楚commit和push都是上传，为什么要进行两次呢。现在清楚了，commit提交的是“历史记录”，未来可以通过commit命令的记录复原文件。</p><h4 id="4-2-关于提交日志"><a href="#4-2-关于提交日志" class="headerlink" title="4.2 关于提交日志"></a>4.2 关于提交日志</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">log</span>                 <span class="comment">#显示以往提交的hash值，提交人和时间。</span></span><br><span class="line">$ git <span class="built_in">log</span> --pretty=short  <span class="comment">#不显示日期</span></span><br><span class="line">$ git <span class="built_in">log</span> README.md       <span class="comment">#只显示与指定文件相关的日志</span></span><br><span class="line">$ git <span class="built_in">log</span> -p</span><br><span class="line">$ git lop -p README.md    <span class="comment">#只看某文件提交前后的差别</span></span><br></pre></td></tr></table></figure><h4 id="4-3-查看工作树、暂存区和最新提交的区别"><a href="#4-3-查看工作树、暂存区和最新提交的区别" class="headerlink" title="4.3 查看工作树、暂存区和最新提交的区别"></a>4.3 查看工作树、暂存区和最新提交的区别</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git diff            <span class="comment">#对比工作区(未add)和暂存区(add之后),如果当前暂存区没有文件，就比较当前工作区和版本库(上一次commit之后)的差别</span></span><br><span class="line">$ git diff HEAD       <span class="comment">#对比工作区(未add)和暂存区（add但未commit），分别和版本库(上一次commit之后)</span></span><br><span class="line">$ git diff --cached   <span class="comment">#对比暂存区(add之后)和版本库(上一次commit之后)</span></span><br></pre></td></tr></table></figure><h4 id="4-4-分支的操作"><a href="#4-4-分支的操作" class="headerlink" title="4.4 分支的操作"></a>4.4 分支的操作</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git branch                    <span class="comment">#查看所有分支，*为当前所在</span></span><br><span class="line">$ git checkout -b feature-A     <span class="comment">#创建名为feature-A的分支并且切换到它</span></span><br><span class="line">$ git branch feature-A          <span class="comment">#效果等同</span></span><br><span class="line">$ git checkout feature-A        <span class="comment">#效果等同</span></span><br></pre></td></tr></table></figure><p>在feature-A分支下对README.md修改，并不会影响master分支下的README.md，这就是Git分支的优点。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -                <span class="comment">#切换回上一个分支</span></span><br><span class="line">$ git checkout master           <span class="comment">#切回master,等同上一条</span></span><br><span class="line">$ git merge --no-ff feature -A  <span class="comment">#合并分支A至master</span></span><br><span class="line">$ git <span class="built_in">log</span> --graph               <span class="comment">#图表输出日志</span></span><br></pre></td></tr></table></figure><ul><li><h2 id="4-3-更改提交"><a href="#4-3-更改提交" class="headerlink" title="4.3 更改提交"></a>4.3 更改提交</h2><p>处理文件的回溯，推进，以及提交信息错误或者是因为出现了简单的错误而不想占用过多历史空间的情况。</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git reset --hard <span class="built_in">hash</span>值       <span class="comment">#回复到某hash值的状态（所有文件）</span></span><br><span class="line">$ git reflog                    <span class="comment">#git log只能以当前状态为终点查看日志，而这可以看所有历史</span></span><br><span class="line">$ git commit --amend            <span class="comment">#修改提交信息（如之前的first commit）（Vim）</span></span><br><span class="line">$ git rebase -i HEAD~2          <span class="comment">#选定当前分支中包含HEAD（最新提交）在内的2个最新历史记录为对象，并在编辑器中打开</span></span><br></pre></td></tr></table></figure><p>rebase提交后会出现：<br>pick hashA aaa<br>pick hashB bbb<br>……</p><p>若想将bbb的历史记录压缩到aaa里，将第二行的pick改成fixup后保存并关闭编辑器即可。</p><p>之后再将aaa分支与master合并，便省去了多合并一次bbb的历史。</p><h2 id="5-推送至远程仓库"><a href="#5-推送至远程仓库" class="headerlink" title="5 推送至远程仓库"></a>5 推送至远程仓库</h2><p>之前的操作都是针对本地仓库在git上进行的，现在开始学习GitHub上的远程仓库操作。</p><h3 id="5-1-添加远程仓库"><a href="#5-1-添加远程仓库" class="headerlink" title="5.1 添加远程仓库"></a>5.1 添加远程仓库</h3><p>首先在GitHub网站上建立新的仓库，注意一般不勾选自动建立README.md的选项，防止本地仓库与远程仓库失去“整合性”（当然也可以后期强制覆盖）。</p><p>然后将该仓库设置成本地仓库的远程仓库：</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin git@github.com:yourname&#x2F;yourrepositories.git</span><br></pre></td></tr></table></figure><p>这一步完成后Git会自动将远程仓库的名字设置为origin。</p><h3 id="5-2-推送至远程仓库"><a href="#5-2-推送至远程仓库" class="headerlink" title="5.2 推送至远程仓库"></a>5.2 推送至远程仓库</h3><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push -u origin master         #将当前分支内容推送给远程仓库的origin的master分支</span><br></pre></td></tr></table></figure><p>注意此处“-u”参数的作用在于，将origin仓库的master分支设置为本地当前分支的上游（upstream），这使得未来通过git pull从远程仓库获取内容时本地仓库的这个分支可以直接从origin-master获得内容。（只有第一次推送带参数-u，之后不用再带）</p><p>本地master以外的分支推送也是同理：</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b feature-B         #切换到分支B</span><br><span class="line">$ git push -u origin feature-B      #推送分支B，即将本地的B分支推送到远程（origin）</span><br></pre></td></tr></table></figure><h3 id="5-3-从远程仓库获取"><a href="#5-3-从远程仓库获取" class="headerlink" title="5.3 从远程仓库获取"></a>5.3 从远程仓库获取</h3><h4 id="获取远程仓库"><a href="#获取远程仓库" class="headerlink" title="获取远程仓库"></a>获取远程仓库</h4><p>首先<strong>切换到其他目录</strong>下，准备将GitHub上的仓库clone到本地：</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone git@github.com:yourname&#x2F;yourrepositories.git</span><br></pre></td></tr></table></figure><p>运行后我出现了Enter passphrase for key ‘/c/Users/Shinelon/.ssh/id_rsa’:，直接输入密码继续。<br>（在其他目录下也要先git init建立仓库数据，否则这一步会报错！）<br>完成后本地仓库的master分支与远程仓库的master分支完全相同。</p><p>使用-a参数可以同时查看本地仓库和远程仓库的分支信息：</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git branch-a</span><br></pre></td></tr></table></figure><h4 id="获取远程某分支"><a href="#获取远程某分支" class="headerlink" title="获取远程某分支"></a>获取远程某分支</h4><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b feature-X origin&#x2F;feature-Y</span><br></pre></td></tr></table></figure><p>以远程仓库的分支Y为来源，在本地仓库中建立X分支。</p><h3 id="5-4-向远程分支提交修改"><a href="#5-4-向远程分支提交修改" class="headerlink" title="5.4 向远程分支提交修改"></a>5.4 向远程分支提交修改</h3><p>一般情况下，如果要对自己/他人的远程分支提交修改，可以如下操作：</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -b featrueX origin&#x2F;featureY          #从远程获取Y分支</span><br><span class="line">$ git diff                                          #在本地对X，即获取到的Y进行修改</span><br><span class="line">$ git commit -am &quot;xxx&quot;                              #加入暂存区并提交，&#x3D;add+commit</span><br><span class="line">$ git push                                          #将本地X分支推送到Y</span><br></pre></td></tr></table></figure><p>获取最新分支</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git pull origin featureY</span><br></pre></td></tr></table></figure><p>将本地的X通过Y更新到最新。</p><p>到这里书中关于推送的内容已经结束了，但是我还有一些问题，所以自己尝试解决一下：</p><h3 id="5-5-一些问题"><a href="#5-5-一些问题" class="headerlink" title="5.5 一些问题"></a>5.5 一些问题</h3><p><strong>1. 在本地建立仓库直接新建文件夹，然后cd到目录下git init即可；远程则是GitHub网页建立新仓库。通过git clone命令能够将origin（远程）的仓库直接克隆到本地，而本地到origin的push命令却只能push某个分支的内容到远程？</strong></p><p>答：确实是这样，要推送全部分支到origin的话需要使用</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push -all origin                             #这样即是将本地所有分支都推送到了origin</span><br></pre></td></tr></table></figure><p><strong>2.练习时创建了大量分支，本地和远程都有，要如何删除？</strong></p><p>答：</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git branch -d &lt;BranchName&gt;                       #删除了本地的BranchName分支</span><br><span class="line">$ git push origin :&lt;BranchName&gt;                    #将一个空分支推送至远程BranchName，效果等同与删除远程BranchName</span><br><span class="line">$ git push origin --delete &lt;BranchName&gt;            #真正的删除指令</span><br></pre></td></tr></table></figure><p><strong>3.分支只能在本地之间切换吗？</strong></p><p>答：是这样，而且研究了一下我发现git branch和git checkout虽然在新建分支并切换的时候效果等同，但实际上branch就是一个新建的命令，checkout才是切换至某分支的指令。</p><p>更多简单的指令可以看看这个网址，感觉说的还比较详细：<a href="https://www.yiibai.com/git/git_push.html" target="_blank" rel="noopener">Git教程</a>。</p><p><strong>4.今天在本地修改文件后同步到GitHub中，直接使用git commit -am ‘说明’进行提交，结果出现了一点小bug，部分GitHub中的文件夹丢失了。具体原因还不是很清楚。</strong></p><p>答：改用</p><figure class="highlight plain"><figcaption><span>.input .Bash&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m &#39; &#39;</span><br><span class="line">$ git push</span><br></pre></td></tr></table></figure><p>另外补充一个git add的使用说明：<img src="D:/lizhe_Notes/gitadd.png" alt=""></p><h2 id="6-快捷施法"><a href="#6-快捷施法" class="headerlink" title="6 快捷施法"></a>6 快捷施法</h2><p><strong>1.显示快捷键：</strong></p><p>【Shift+/】</p><p><strong>2.通过URL直接查看分支间的差别：</strong></p><p>①分支4-0-stable和3-2-stable之间的差别：</p><p><a href="https://github.com/rails/rails/" target="_blank" rel="noopener">https://github.com/rails/rails/</a> <strong>compare/4-0-stabel…3-2-stable</strong></p><p>②查看master分支在最近7天内的差别：</p><p><a href="https://github.com/rails/rails/" target="_blank" rel="noopener">https://github.com/rails/rails/</a> <strong>compare/master@{7.day.ago}…master</strong></p><p>③查看master分支在2013年9月9日与现在的区别<br><a href="https://github.com/rails/rails/" target="_blank" rel="noopener">https://github.com/rails/rails/</a> <strong>compare/master@{2013-09-09}…master</strong></p><h3 id="6-1-Issue的使用"><a href="#6-1-Issue的使用" class="headerlink" title="6.1 Issue的使用"></a>6.1 Issue的使用</h3><p><strong>1.给Issue添加标签/里程碑，以及GFM语法。（略）</strong></p><p><strong>2.GFM的独有功能Tasklist语法：</strong></p><blockquote><p>本日要做的任务</p></blockquote><ul><li>[x] 早上8点以前起床</li><li>[ ] 看完《怪诞心理学》并还回图书馆</li><li>[ ] 学习时间超过5h</li><li>[ ] 运动至少30min</li></ul><p><strong>3.Close Issue：</strong></p><p>若一个处于OPEN状态的Issue处理完毕，可在该提交中以以下任意格式描述提交信息，从而Close该Issue：</p><p>fix#24 fixes#24 fixed#24 /fix可以换成close和resolve</p><h3 id="6-2-Pull-Request"><a href="#6-2-Pull-Request" class="headerlink" title="6.2 Pull Request"></a>6.2 Pull Request</h3><p><strong>1.以.diff或.patch格式获取Pull Request：</strong></p><p>如下输入即可：</p><p><a href="https://github.com/UserName/RepositoryName" target="_blank" rel="noopener">https://github.com/UserName/RepositoryName</a> <strong>/pull/28.diff</strong> 28为Pull Request的编号#28</p><p><a href="https://github.com/UserName/RepositoryName" target="_blank" rel="noopener">https://github.com/UserName/RepositoryName</a> <strong>/pull/28.patch</strong></p><p><strong>2.在Conversation引用评论：</strong></p><p>选中想要引用的部分，然后按【R】。</p><p><strong>3.Files Changed：</strong></p><p>该标签页可以查看当前Pull Request更改的内容以及前后的差别，但是默认情况下空格的改变也会高亮显示，不方便，可以在URL末尾添加：</p><p>“?w=1”</p><p>便不再显示空格的区别。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;这篇文章是我入门Github时期记录的笔记，当时主要是参考&lt;a href=&quot;https://book.douban.com/subject/26462816/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Github入门与实践
      
    
    </summary>
    
    
      <category term="学习笔记" scheme="http://li-zhe.com/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="GitHub" scheme="http://li-zhe.com/tags/GitHub/"/>
    
  </entry>
  
</feed>
